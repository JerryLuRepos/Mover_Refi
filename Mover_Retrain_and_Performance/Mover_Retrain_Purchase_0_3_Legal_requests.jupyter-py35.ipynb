{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reminder\n",
    "\n",
    "Preprocess was not saved in MoverV3.2 code. There are 6 columns with different imputation between Mover_V3 and MoverV3.2\n",
    "\n",
    "Here, only preprocessed data are used to retrain MoverV3.2 for legal requests by removing three columns:\n",
    "    1. p_31_50_exp\n",
    "    2. p_married_exp\n",
    "    3. borrowerage_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success configuring sparkmagic livy.\n",
      "['https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1', 'https://qlawsdl001038a.ad.datalake.foc.zone:8443/gateway/dsx/livy/v1']\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "from dsx_core_utils import proxy_util,dsxhi_util\n",
    "proxy_util.configure_proxy_livy() \n",
    "dsxhi_util.list_livy_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\"executorCores\": 2, \"numExecutors\": 10, \"executorMemory\": \"15g\", \n",
    " \"driverMemory\": \"12g\", \"proxyUser\": \"aliu-\", \"driverCores\": 1, \n",
    " \"conf\": {\"spark.yarn.appMasterEnv.THEANO_FLAGS\": \"base_compiledir=${PWD}/.theano\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>22880</td><td>application_1590030838276_68049</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://qlawsbidlhm02b.ad.datalake.foc.zone:8090/proxy/application_1590030838276_68049/\">Link</a></td><td><a target=\"_blank\" href=\"https://qlawsbidlhw14b.ad.datalake.foc.zone:8042/node/containerlogs/container_e499_1590030838276_68049_01_000001/aliu-\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s inactivePurchaseLeadGen -k -l python -u https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "import pyspark\n",
    "import os, sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lower\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import last_day, rand, dense_rank, last_day, col, size, length, when, upper, unix_timestamp, avg, substring, lower, udf, sum, count, lit, mean, concat, countDistinct, desc, from_unixtime, row_number, year, month, to_date, upper, months_between\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, QuantileDiscretizer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from os.path import expanduser, join, abspath\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "model_save_path = '/dev/projects/retention_models/purchase_payoff/models/'\n",
    "training_data_path = '/dev/projects/retention_models/purchase_payoff/training/data/train/'\n",
    "test_data_path = '/dev/projects/retention_models/purchase_payoff/training/data/test/'\n",
    "result_path = '/dev/projects/retention_models/purchase_payoff/training/results/movermodel/'\n",
    "\n",
    "monthly_prep_path = '/dev/projects/retention_models/monthly_snapshot/monthly_preprocessed/'\n",
    "actual_path = '/dev/projects/retention_models/actual_value/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Load Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data for Mover Purchase V3.2 and Refi V1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df201612 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201612.parquet').drop('gcid')\n",
    "df201703 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201703.parquet').drop('gcid')\n",
    "df201706 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201706.parquet').drop('gcid')\n",
    "df201709 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201709.parquet').drop('gcid')\n",
    "df201712 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201712.parquet').drop('gcid')\n",
    "df201803 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201803.parquet').drop('gcid')\n",
    "df201806 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201806.parquet').drop('gcid')\n",
    "df201809 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201809.parquet').drop('gcid')\n",
    "df201812 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201812.parquet')\n",
    "df201903 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201903.parquet')\n",
    "df201906 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201906.parquet')\n",
    "df201909 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201909.parquet')\n",
    "df201912 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201912.parquet')\n",
    "df202003 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202003.parquet')\n",
    "\n",
    "\n",
    "df_preprocessed = df201612.union(df201703).union(df201706).union(df201709).union(df201712)\\\n",
    "                            .union(df201803).union(df201806).union(df201809).union(df201812)\\\n",
    "                            .union(df201903).union(df201906).union(df201909).union(df201912)\\\n",
    "                            .union(df202003)\\\n",
    "                            .drop('borrowerage_bucket','p_married_exp','p_31_50_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- servicecalendardate: date (nullable = true)\n",
      " |-- ln_no: string (nullable = true)\n",
      " |-- og_note_dt: timestamp (nullable = true)\n",
      " |-- ct_age_exp: integer (nullable = true)\n",
      " |-- ct_1_exp: string (nullable = true)\n",
      " |-- ct_2_exp: string (nullable = true)\n",
      " |-- ct_3_exp: string (nullable = true)\n",
      " |-- home_value_exp: double (nullable = true)\n",
      " |-- p_edu_hs_exp: integer (nullable = true)\n",
      " |-- personnum_per_room_exp: double (nullable = true)\n",
      " |-- mosaic_group_refi_exp: string (nullable = true)\n",
      " |-- mosaic_group_pur_exp: string (nullable = true)\n",
      " |-- ratespread_min_exp: double (nullable = true)\n",
      " |-- ratespread_min_pur_exp: double (nullable = true)\n",
      " |-- ln_ann_int_rt: double (nullable = true)\n",
      " |-- loantypedescription_exp: string (nullable = true)\n",
      " |-- loanamortizationtype: string (nullable = true)\n",
      " |-- ageinmon_exp: double (nullable = true)\n",
      " |-- og_mtg_am_exp: double (nullable = true)\n",
      " |-- currentcltv_exp: double (nullable = true)\n",
      " |-- orig_fico_exp: integer (nullable = true)\n",
      " |-- LiveYears_short_exp: double (nullable = true)\n",
      " |-- LiveYears_long_exp: double (nullable = true)\n",
      " |-- LiveYears_grp_exp: double (nullable = true)\n",
      " |-- ln_purpose_type_exp: string (nullable = true)\n",
      " |-- ln_purpose_type: string (nullable = true)\n",
      " |-- og_occupy_stat_type_exp: string (nullable = true)\n",
      " |-- issingleborrower_exp: integer (nullable = true)\n",
      " |-- ln_tr_exp: integer (nullable = true)\n",
      " |-- investornameshort: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_preprocessed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Actual Payoff Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------------+\n",
      "|servicecalendardate|     ln_no|purchasepayoff|\n",
      "+-------------------+----------+--------------+\n",
      "|         2016-10-31|3219834719|           0.0|\n",
      "+-------------------+----------+--------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "actual = spark.read.parquet(actual_path + 'Actual_payoff_20200610.parquet')\\\n",
    "                .select('servicecalendardate', 'ln_no', 'purchasepayoff')\n",
    "\n",
    "actual.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|servicecalendardate|  count|\n",
      "+-------------------+-------+\n",
      "|         2020-05-31|1889579|\n",
      "|         2020-04-30|1899099|\n",
      "|         2020-03-31|1867297|\n",
      "|         2020-02-29|1846864|\n",
      "|         2020-01-31|1824706|\n",
      "|         2019-12-31|1802243|\n",
      "|         2019-11-30|1807991|\n",
      "|         2019-10-31|1787277|\n",
      "|         2019-09-30|1763391|\n",
      "|         2019-08-31|1856202|\n",
      "|         2019-07-31|1839005|\n",
      "|         2019-06-30|1822001|\n",
      "|         2019-05-31|1806604|\n",
      "|         2019-04-30|1788444|\n",
      "|         2019-03-31|1770015|\n",
      "|         2019-02-28|1752593|\n",
      "|         2019-01-31|1738794|\n",
      "|         2018-12-31|1726017|\n",
      "|         2018-11-30|1713615|\n",
      "|         2018-10-31|1699510|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "actual.groupBy('servicecalendardate').count().sort(col(\"servicecalendardate\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867297"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df202003.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Join actual data with preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Model is retrained in 2020-05, 2020-03 data cannot be used for not seasoned enough!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------------+\n",
      "|servicecalendardate|count(ln_no)|sum(purchasepayoff)|\n",
      "+-------------------+------------+-------------------+\n",
      "|         2020-03-31|     1867297|             4440.0|\n",
      "|         2019-12-31|     1802243|            13866.0|\n",
      "|         2019-09-30|     1763391|            14160.0|\n",
      "|         2019-06-30|     1822001|            19544.0|\n",
      "|         2018-12-31|     1726017|            14249.0|\n",
      "|         2018-09-30|     1683583|            12477.0|\n",
      "|         2018-06-30|     1619695|            17312.0|\n",
      "|         2018-03-31|     1577779|            19761.0|\n",
      "|         2017-12-31|     1530004|            12739.0|\n",
      "|         2017-09-30|     1468441|            11685.0|\n",
      "|         2017-06-30|     1423608|            15272.0|\n",
      "|         2017-03-31|     1379094|            16819.0|\n",
      "|         2016-12-31|     1326375|             9786.0|\n",
      "+-------------------+------------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all.groupby('servicecalendardate').agg(count('ln_no'), sum('purchasepayoff')).orderBy(col('servicecalendardate').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0064146092488312195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(11487/(1802243-11487))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Under-sample negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# create fundtion for undersampling\n",
    "def data_split(df, train_months, target, rate):\n",
    "    \n",
    "    ## Split Train/Validation From Test ## \n",
    "    train_validate = df.where(col('servicecalendardate').isin(train_months))\n",
    "    \n",
    "    ## Split Out Payoffs into Train/Validate ##\n",
    "    train_refi_all = train_validate.filter(train_validate[target] == '1.')\n",
    "    train_refi, validate_refi = train_refi_all.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "    ## Split Out Non-Payoffs to Train/Validate\n",
    "    train_nonrefi_all = train_validate.filter(train_validate[target] == '0.')\n",
    "    train_nonrefi, validate_nonrefi = train_nonrefi_all.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "    ## Undersample Non-Payoffs ## \n",
    "    train_non, notused1 = train_nonrefi.randomSplit([rate, 1-rate], seed=123)\n",
    "    val_non, notused2 = validate_nonrefi.randomSplit([rate, 1-rate], seed=123)\n",
    "\n",
    "    ## Create Final Training/Validate Sets\n",
    "    train = train_refi.unionAll(train_non)\n",
    "    validate = validate_refi.unionAll(val_non)\n",
    "    train.cache()\n",
    "    \n",
    "    return train, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "train_months = ['2017-12-31', '2018-03-31', '2018-06-30', '2018-09-30', '2018-12-31', '2019-03-31', '2019-06-30']\n",
    "\n",
    "target = 'purchasepayoff'\n",
    "df_train, df_validate = data_split(df_all, train_months, target, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----+\n",
      "|servicecalendardate|purchasepayoff|count|\n",
      "+-------------------+--------------+-----+\n",
      "|         2017-12-31|           0.0|26450|\n",
      "|         2017-12-31|           1.0| 8924|\n",
      "|         2018-03-31|           0.0|27228|\n",
      "|         2018-03-31|           1.0|13888|\n",
      "|         2018-06-30|           0.0|27894|\n",
      "|         2018-06-30|           1.0|12151|\n",
      "|         2018-09-30|           0.0|29512|\n",
      "|         2018-09-30|           1.0| 8783|\n",
      "|         2018-12-31|           0.0|29996|\n",
      "|         2018-12-31|           1.0|10015|\n",
      "|         2019-06-30|           0.0|31876|\n",
      "|         2019-06-30|           1.0|13483|\n",
      "+-------------------+--------------+-----+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_train.groupby(['servicecalendardate', 'purchasepayoff']).count().orderBy('servicecalendardate', 'purchasepayoff').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def create_pipeline(target, *arg):\n",
    "    \n",
    "    exclude_cols = ('servicecalendardate', 'ln_no', 'og_note_dt', 'mosaic_group_pur_exp', 'ratespread_min_pur_exp',\n",
    "                'ln_purpose_type', 'investornameshort', 'ln_ann_int_rt', target) \n",
    "    cat_cols = [i[0] for i in df_train.dtypes if ((i[1]=='string') & (~i[0].endswith(exclude_cols)))]\n",
    "    num_cols = [i[0] for i in df_train.dtypes if ((i[1].startswith(('int', 'double'))) & (~i[0].endswith(exclude_cols)))]\n",
    "    \n",
    "    stages = []\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        \n",
    "        #Category indexing with StringIndexer\n",
    "        indexer = StringIndexer(inputCol = col, outputCol = col+'_idx').setHandleInvalid('keep')\n",
    "        stages += [indexer]\n",
    "        \n",
    "    #assemblerInputs = [c+'_vec' for c in cat_cols] + num_cols\n",
    "    assemblerInputs = [c+'_idx' for c in cat_cols] + num_cols\n",
    "    assembler = VectorAssembler(inputCols = assemblerInputs, outputCol = 'vectFeatures')\n",
    "    \n",
    "    stages += [assembler]\n",
    "    \n",
    "    lr = LogisticRegression(maxIter=100, regParam=0.1, elasticNetParam=0.0, fitIntercept = True,\n",
    "                            featuresCol='vectFeatures', labelCol=target)\n",
    "    rf = RandomForestClassifier(numTrees=250, maxDepth = 5, featuresCol='vectFeatures', labelCol=target)\n",
    "    gbt = GBTClassifier(maxIter=100, featuresCol='vectFeatures', labelCol=target)\n",
    "\n",
    "    pipeline_lr = Pipeline(stages = stages + [lr])\n",
    "    pipeline_rf = Pipeline(stages = stages + [rf])\n",
    "    pipeline_gbt = Pipeline(stages = stages + [gbt])\n",
    " \n",
    "    if \"lr\" in arg and \"rf\" in arg and \"gbt\" in arg:\n",
    "        return lr, rf, gbt, pipeline_lr, pipeline_rf, pipeline_gbt\n",
    "    elif \"lr\" in arg and \"rf\" in arg:\n",
    "        return lr, rf, pipeline_lr, pipeline_rf\n",
    "    elif \"lr\" in arg and \"gbt\" in arg:\n",
    "        return lr, gbt, pipeline_lr, pipeline_gbt \n",
    "    elif \"rf\" in arg and \"gbt\" in arg:\n",
    "        return rf, gbt, pipeline_rf, pipeline_gbt\n",
    "    elif \"rf\" in arg:\n",
    "        return rf, pipeline_rf\n",
    "    elif \"lr\" in arg:\n",
    "        return lr, pipeline_lr\n",
    "    elif \"gbt\" in arg:\n",
    "        return gbt, pipeline_gbt\n",
    "    else:\n",
    "        return gbt, pipeline_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fit_pipeline(pipeline, training_dataset):\n",
    "    model_pipeline = pipeline.fit(training_dataset)\n",
    "    return model_pipeline\n",
    "    \n",
    "def persist_modelpersist_(pipeline_model, model_name, model_save_path=model_save_path):\n",
    "    pipeline_model.save(model_save_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "lr, rf, gbt, pipeline_lr, pipeline_rf, pipeline_gbt = create_pipeline(target, \"lr\", \"rf\", \"gbt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Time: 88.2570116519928"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Logistic Regression\n",
    "start = time.time()\n",
    "model_pipeline_lr = fit_pipeline(pipeline_lr, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Logistic Regression Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Time: 106.32498598098755"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Random Forest\n",
    "start = time.time()\n",
    "model_pipeline_rf = fit_pipeline(pipeline_rf, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Random Forest Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Training Time: 1286.2877581119537"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Gradient Boosting\n",
    "start = time.time()\n",
    "model_pipeline_gbt = fit_pipeline(pipeline_gbt, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Gradient Boosting Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "persist_modelpersist_(model_pipeline_lr, 'MoverV3_3_LR_20200513')\n",
    "persist_modelpersist_(model_pipeline_rf, 'MoverV3_3_RF_20200513')\n",
    "persist_modelpersist_(model_pipeline_gbt, 'MoverV3_3_GBT_20200513')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "model_pipeline_lr = PipelineModel.load(path = model_save_path+'MoverV3_3_LR_20200513')\n",
    "model_pipeline_rf = PipelineModel.load(path = model_save_path+'MoverV3_3_RF_20200513')\n",
    "model_pipeline_gbt = PipelineModel.load(path = model_save_path+'MoverV3_3_GBT_20200513')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "def extractFeatureImp(model_pipeline, df_train, featuresCol):\n",
    "    \n",
    "    featureImp = model_pipeline.stages[-1].featureImportances\n",
    "    transformed = model_pipeline.transform(df_train)\n",
    "    list_extract = []\n",
    "    \n",
    "    for i in transformed.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + transformed.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "        \n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    \n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "    return varlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx                         name  \\\n",
      "5    13                 ageinmon_exp   \n",
      "7    15              currentcltv_exp   \n",
      "4    12           ratespread_min_exp   \n",
      "10   18           LiveYears_long_exp   \n",
      "6    14                og_mtg_am_exp   \n",
      "0     8                   ct_age_exp   \n",
      "8    16                orig_fico_exp   \n",
      "17    3    mosaic_group_refi_exp_idx   \n",
      "1     9               home_value_exp   \n",
      "13   21                    ln_tr_exp   \n",
      "20    6      ln_purpose_type_exp_idx   \n",
      "3    11       personnum_per_room_exp   \n",
      "9    17          LiveYears_short_exp   \n",
      "21    7  og_occupy_stat_type_exp_idx   \n",
      "16    2                 ct_3_exp_idx   \n",
      "2    10                 p_edu_hs_exp   \n",
      "19    5     loanamortizationtype_idx   \n",
      "18    4  loantypedescription_exp_idx   \n",
      "11   19            LiveYears_grp_exp   \n",
      "14    0                 ct_1_exp_idx   \n",
      "15    1                 ct_2_exp_idx   \n",
      "12   20         issingleborrower_exp   \n",
      "\n",
      "                                                 vals     score  \n",
      "5                                                 NaN  0.102705  \n",
      "7                                                 NaN  0.100361  \n",
      "4                                                 NaN  0.096415  \n",
      "10                                                NaN  0.085103  \n",
      "6                                                 NaN  0.072984  \n",
      "0                                                 NaN  0.068485  \n",
      "8                                                 NaN  0.061783  \n",
      "17    [other, DEFJK, ACG, BHQ, IL, MNOPRS, __unknown]  0.058023  \n",
      "1                                                 NaN  0.052297  \n",
      "13                                                NaN  0.037983  \n",
      "20     [refinance, cashout refi, purchase, __unknown]  0.037416  \n",
      "3                                                 NaN  0.032006  \n",
      "9                                                 NaN  0.031839  \n",
      "21  [primary residence, investment property, secon...  0.025576  \n",
      "16                            [0, 1, 2, 3, __unknown]  0.025177  \n",
      "2                                                 NaN  0.023078  \n",
      "19                            [fixed, arm, __unknown]  0.021781  \n",
      "18                         [conv, fha, va, __unknown]  0.021195  \n",
      "11                                                NaN  0.017820  \n",
      "14                                  [0, 1, __unknown]  0.010869  \n",
      "15                               [0, 1, 2, __unknown]  0.010162  \n",
      "12                                                NaN  0.006944"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "gbt_feature_importance = extractFeatureImp(model_pipeline_gbt, df_train, \"vectFeatures\")\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(gbt_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def predict(model_pipeline, data):\n",
    "    \n",
    "    _scoreUdf = udf(lambda v: float(v[1]), DoubleType())\n",
    "    \n",
    "    prediction = model_pipeline.transform(data)\n",
    "    pred_df = prediction.withColumn('pred', _scoreUdf(prediction['probability']))\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def roc_auc(df, target):   \n",
    "    \n",
    "    pred_df_lr = predict(model_pipeline_lr, df)\n",
    "    pred_df_rf = predict(model_pipeline_rf, df)\n",
    "    pred_df_gbt = predict(model_pipeline_gbt, df)\n",
    "\n",
    "    ### Calculate ROC ###  \n",
    "    evalPred_lr = pred_df_lr.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "    evalPred_rf = pred_df_rf.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "    evalPred_gbt = pred_df_gbt.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "\n",
    "    evaluatorLR = BinaryClassificationEvaluator()\n",
    "    evaluatorRF = BinaryClassificationEvaluator()\n",
    "    evaluatorGBT = BinaryClassificationEvaluator()\n",
    "\n",
    "    print(\"Test Area Under ROC - LR: \" + str(evaluatorLR.evaluate(evalPred_lr, {evaluatorLR.metricName: \"areaUnderROC\"})))        \n",
    "    print(\"Test Area Under ROC - RF: \" + str(evaluatorRF.evaluate(evalPred_rf, {evaluatorRF.metricName: \"areaUnderROC\"})))        \n",
    "    print(\"Test Area Under ROC - GBT: \" + str(evaluatorGBT.evaluate(evalPred_gbt, {evaluatorGBT.metricName: \"areaUnderROC\"})))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC - LR: 0.6309517205196433\n",
      "Test Area Under ROC - RF: 0.6587607092973101\n",
      "Test Area Under ROC - GBT: 0.7061909966675342\n",
      "\n",
      "\n",
      "Test Area Under ROC - LR: 0.6268778748195674\n",
      "Test Area Under ROC - RF: 0.6546481944596021\n",
      "Test Area Under ROC - GBT: 0.6878066684733987\n",
      "\n",
      "\n",
      "Test Area Under ROC - LR: 0.6276730125751022\n",
      "Test Area Under ROC - RF: 0.668378382185012\n",
      "Test Area Under ROC - GBT: 0.6907329097345398"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "test_months = ['2017-09-30', '2019-09-30', '2019-12-31']\n",
    "df_test = df_all.where(col('servicecalendardate').isin(test_months))\n",
    "\n",
    "for df in [df_train, df_validate, df_test]:\n",
    "    roc_auc(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "test_months = ['2017-09-30', '2019-09-30', '2019-12-31']\n",
    "df_test = df_all.where(col('servicecalendardate').isin(test_months))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Capture Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def deciles(df, model, target):\n",
    "    \n",
    "    pred_df = predict(model, df)\n",
    "\n",
    "    pred_df = QuantileDiscretizer(numBuckets=10, inputCol=\"pred\", outputCol=\"decile\", relativeError=0.00001,\n",
    "                             handleInvalid=\"error\").fit(pred_df).transform(pred_df)\n",
    "    pred_df = pred_df.withColumn('decile', (10 - F.col('decile')).cast('int'))\n",
    "    \n",
    "    window_cumsum = Window.orderBy('decile').rangeBetween(Window.unboundedPreceding, 0)\n",
    "    total_target = pred_df.select(F.sum(target)).collect()[0][0]\n",
    "    df_out = pred_df\\\n",
    "        .groupBy('decile', )\\\n",
    "        .agg(F.count('ln_no').alias('decile_cnt'), F.sum(target).alias('payoff_cnt'))\\\n",
    "        .withColumn('cum_sum', F.sum('payoff_cnt').over(window_cumsum) / total_target)\\\n",
    "        .sort('decile')\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'test_months' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'test_months' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "target = 'purchasepayoff'\n",
    "date_label = test_months[2]\n",
    "print(date_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "#print(date_label)\n",
    "#deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_lr, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "#print(date_label)\n",
    "#deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_rf, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    180219|    3510.0| 0.2531371700562527|\n",
      "|     2|    180218|    2206.0|0.41223135727679217|\n",
      "|     3|    180233|    1796.0| 0.5417568152315015|\n",
      "|     4|    180213|    1473.0| 0.6479878840328862|\n",
      "|     5|    180243|    1415.0|  0.750036059425934|\n",
      "|     6|    180207|    1143.0| 0.8324679071109188|\n",
      "|     7|    180232|     867.0| 0.8949949516803692|\n",
      "|     8|    180221|     702.0| 0.9456223856916198|\n",
      "|     9|    180198|     516.0| 0.9828357132554449|\n",
      "|    10|    180259|     238.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(date_label)\n",
    "deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "actual1 = spark.read.parquet(actual_path + 'Actual_payoff_20200505.parquet')\\\n",
    "                .select('servicecalendardate', 'ln_no', 'purchasepayoff')\n",
    "\n",
    "df_all1 = df_preprocessed.join(actual1, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates().where(col('servicecalendardate') == '2019-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    180219|    2934.0|0.25541916949595195|\n",
      "|     2|    180232|    1876.0| 0.4187342212936363|\n",
      "|     3|    180219|    1461.0|  0.545921476451641|\n",
      "|     4|    180229|    1214.0| 0.6516061634891617|\n",
      "|     5|    180237|    1165.0| 0.7530251588752502|\n",
      "|     6|    180201|     941.0| 0.8349438495690781|\n",
      "|     7|    180207|     731.0| 0.8985810046139113|\n",
      "|     8|    180225|     565.0| 0.9477670410028728|\n",
      "|     9|    180212|     410.0| 0.9834595629842431|\n",
      "|    10|    180262|     190.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "deciles(df_all1.filter(col('servicecalendardate') == date_label), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fullMonthPrediction(df):\n",
    "    \n",
    "    pred_df_lr = predict(model_pipeline_lr, df)\n",
    "    validationPredictionsLR = pred_df_lr.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'logRegProb')\n",
    "    \n",
    "    pred_df_rf = predict(model_pipeline_rf, df)\n",
    "    validationPredictionsRF = pred_df_rf.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'randForProb')\n",
    "    \n",
    "    pred_df_gbt = predict(model_pipeline_gbt, df)\n",
    "    validationPredictionsGBT = pred_df_gbt.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'gbtProb')\n",
    "    \n",
    "    combinedFinalPred = validationPredictionsLR.join(validationPredictionsRF, on='ln_no', how='left')\\\n",
    "                                                .join(validationPredictionsGBT, on='ln_no', how='left')\\\n",
    "                                                .dropDuplicates()\n",
    "    \n",
    "    return combinedFinalPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def pred_save(df, servicedate, filename, result_path = result_path):\n",
    "    \n",
    "    df1 = df.where(col('servicecalendardate') == servicedate)\n",
    "    df_pred = fullMonthPrediction(df1)\n",
    "    \n",
    "    df_pred.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def pred_save_newmonth(preprocessed_file, filename, monthly_prep_path=monthly_prep_path, result_path=result_path):\n",
    "    \n",
    "    df = spark.read.parquet(monthly_prep_path + preprocessed_file)\n",
    "    df_pred = fullMonthPrediction(df)\n",
    "    \n",
    "    print(df.count())\n",
    "    df_pred.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 2017-9\n",
    "pred_save(df_test.where(col('servicecalendardate') == '2017-09-30'), '2017-09-30', 'pred_MoverV3_3_sep17_20200514.csv')\n",
    "\n",
    "\n",
    "# 2019-9\n",
    "pred_save(df_test.where(col('servicecalendardate') == '2019-09-30'), '2019-09-30', 'pred_MoverV3_3_sep19_20200514.csv')\n",
    "\n",
    "          \n",
    "# 2019-12\n",
    "pred_save(df_test.where(col('servicecalendardate') == '2019-12-31'), '2019-12-31', 'pred_MoverV3_3_dec19_20200514.csv')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468441\n",
      "1468441\n",
      "1763391\n",
      "1763391\n",
      "1802243\n",
      "1802243"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "print(df_all.where(col('servicecalendardate') == '2017-09-30').count())\n",
    "print(spark.read.csv(result_path + 'pred_MoverV3_3_sep17_20200514.csv', header = True).count())\n",
    "\n",
    "\n",
    "print(df_all.where(col('servicecalendardate') == '2019-09-30').count())\n",
    "print(spark.read.csv(result_path + 'pred_MoverV3_3_sep19_20200514.csv', header = True).count())\n",
    "\n",
    "\n",
    "print(df_all.where(col('servicecalendardate') == '2019-12-31').count())\n",
    "print(spark.read.csv(result_path + 'pred_MoverV3_3_dec19_20200514.csv', header = True).count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867297\n",
      "+------+----------+----------+------------------+\n",
      "|decile|decile_cnt|payoff_cnt|           cum_sum|\n",
      "+------+----------+----------+------------------+\n",
      "|     1|    186702|    1329.0|0.2993243243243243|\n",
      "|     2|    186761|     693.0|0.4554054054054054|\n",
      "|     3|    186723|     561.0|0.5817567567567568|\n",
      "|     4|    186706|     490.0|0.6921171171171171|\n",
      "|     5|    186748|     397.0|0.7815315315315315|\n",
      "|     6|    186739|     314.0|0.8522522522522522|\n",
      "|     7|    186684|     262.0|0.9112612612612613|\n",
      "|     8|    186779|     192.0|0.9545045045045045|\n",
      "|     9|    186715|     143.0|0.9867117117117117|\n",
      "|    10|    186740|      59.0|               1.0|\n",
      "+------+----------+----------+------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df202003 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202003.parquet')\n",
    "df_preprocessed = df202003.drop('borrowerage_bucket','p_married_exp','p_31_50_exp')\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "\n",
    "pred_save(df_all.where(col('servicecalendardate') == '2020-03-31'), '2020-03-31', 'pred_MoverV3_3_mar20_20200514.csv')\n",
    "print(spark.read.csv(result_path + 'pred_MoverV3_3_mar20_20200514.csv', header = True).count())\n",
    "\n",
    "deciles(df_all.filter(col('servicecalendardate') == '2020-03-31'), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------------+\n",
      "|servicecalendardate|count(ln_no)|sum(purchasepayoff)|\n",
      "+-------------------+------------+-------------------+\n",
      "|         2020-03-31|     1867297|             4440.0|\n",
      "+-------------------+------------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all.groupby('servicecalendardate').agg(count('ln_no'), sum(target)).orderBy(col('servicecalendardate').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899099\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    189876|     222.0| 0.2971887550200803|\n",
      "|     2|    189928|     118.0|0.45515394912985274|\n",
      "|     3|    189893|      80.0| 0.5622489959839357|\n",
      "|     4|    189933|      83.0| 0.6733601070950469|\n",
      "|     5|    189926|      73.0| 0.7710843373493976|\n",
      "|     6|    189900|      57.0| 0.8473895582329317|\n",
      "|     7|    189901|      52.0| 0.9170013386880856|\n",
      "|     8|    189915|      26.0| 0.9518072289156626|\n",
      "|     9|    189918|      24.0| 0.9839357429718876|\n",
      "|    10|    189909|      12.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df202004 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202004.parquet')\n",
    "df_preprocessed = df202004.drop('borrowerage_bucket','p_married_exp','p_31_50_exp')\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "\n",
    "pred_save(df_all.where(col('servicecalendardate') == '2020-04-30'), '2020-04-30', 'pred_MoverV3_3_apr20_20200514.csv')\n",
    "print(spark.read.csv(result_path + 'pred_MoverV3_3_apr20_20200514.csv', header = True).count())\n",
    "\n",
    "deciles(df_all.filter(col('servicecalendardate') == '2020-04-30'), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------------+\n",
      "|servicecalendardate|count(ln_no)|sum(purchasepayoff)|\n",
      "+-------------------+------------+-------------------+\n",
      "|         2020-04-30|     1899099|              747.0|\n",
      "+-------------------+------------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all.groupby('servicecalendardate').agg(count('ln_no'), sum(target)).orderBy(col('servicecalendardate').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+------------------+\n",
      "|decile|decile_cnt|payoff_cnt|           cum_sum|\n",
      "+------+----------+----------+------------------+\n",
      "|     1|    189895|     213.0| 0.285140562248996|\n",
      "|     2|    189910|     125.0|0.4524765729585007|\n",
      "|     3|    189916|      88.0| 0.570281124497992|\n",
      "|     4|    189895|      83.0|0.6813922356091031|\n",
      "|     5|    189886|      55.0|0.7550200803212851|\n",
      "|     6|    189955|      63.0|0.8393574297188755|\n",
      "|     7|    189914|      41.0|0.8942436412315931|\n",
      "|     8|    189890|      39.0|0.9464524765729585|\n",
      "|     9|    189912|      23.0|0.9772423025435074|\n",
      "|    10|    189926|      17.0|               1.0|\n",
      "+------+----------+----------+------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all = spark.read.csv(result_path + 'pred_MoverV3_2_apr20_20200505.csv', header = True)\n",
    "\n",
    "df = df_all.withColumn('pred', col('gbtProb').cast('double'))\\\n",
    "                .withColumn('servicecalendardate', lit('2020-04-30'))\\\n",
    "                .join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "pred_df = QuantileDiscretizer(numBuckets=10, inputCol=\"pred\", outputCol=\"decile\", relativeError=0.00001,\n",
    "                             handleInvalid=\"error\").fit(df).transform(df)\n",
    "pred_df = pred_df.withColumn('decile', (10 - F.col('decile')).cast('int'))\n",
    "    \n",
    "window_cumsum = Window.orderBy('decile').rangeBetween(Window.unboundedPreceding, 0)\n",
    "total_target = pred_df.select(F.sum(target)).collect()[0][0]\n",
    "df_out = pred_df\\\n",
    "        .groupBy('decile', )\\\n",
    "        .agg(F.count('ln_no').alias('decile_cnt'), F.sum(target).alias('payoff_cnt'))\\\n",
    "        .withColumn('cum_sum', F.sum('payoff_cnt').over(window_cumsum) / total_target)\\\n",
    "        .sort('decile')\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846864\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    184663|    2568.0|0.28028814669286184|\n",
      "|     2|    184706|    1413.0|0.43451211525867717|\n",
      "|     3|    184686|    1208.0| 0.5663610565378738|\n",
      "|     4|    184684|    1000.0| 0.6755075311067452|\n",
      "|     5|    184656|     823.0| 0.7653350796769265|\n",
      "|     6|    184691|     699.0| 0.8416284654005676|\n",
      "|     7|    184706|     571.0| 0.9039511023793931|\n",
      "|     8|    184700|     438.0| 0.9517572582405588|\n",
      "|     9|    184651|     320.0| 0.9866841301025977|\n",
      "|    10|    184721|     122.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df202002 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202002.parquet')\n",
    "df_preprocessed = df202002.drop('borrowerage_bucket','p_married_exp','p_31_50_exp')\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "\n",
    "pred_save(df_all.where(col('servicecalendardate') == '2020-02-29'), '2020-02-29', 'pred_MoverV3_3_feb20_20200514.csv')\n",
    "print(spark.read.csv(result_path + 'pred_MoverV3_3_feb20_20200514.csv', header = True).count())\n",
    "\n",
    "deciles(df_all.filter(col('servicecalendardate') == '2020-02-29'), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+------------------+\n",
      "|decile|decile_cnt|payoff_cnt|           cum_sum|\n",
      "+------+----------+----------+------------------+\n",
      "|     1|    184669|    2299.0|0.2509277450338354|\n",
      "|     2|    184688|    1441.0|0.4082078148875791|\n",
      "|     3|    184695|    1177.0|0.5366732154551408|\n",
      "|     4|    184675|    1021.0|0.6481117659899586|\n",
      "|     5|    184706|     863.0|0.7423051735428946|\n",
      "|     6|    184672|     710.0|0.8197991704867933|\n",
      "|     7|    184693|     632.0|  0.88877974241432|\n",
      "|     8|    184691|     504.0|0.9437895655970312|\n",
      "|     9|    184654|     344.0| 0.981335952848723|\n",
      "|    10|    184721|     171.0|               1.0|\n",
      "+------+----------+----------+------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all = spark.read.csv(result_path + 'pred_MoverV3_2_feb20_20200305.csv', header = True)\n",
    "\n",
    "df = df_all.withColumn('pred', col('gbtProb').cast('double'))\\\n",
    "                .withColumn('servicecalendardate', lit('2020-02-29'))\\\n",
    "                .join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "pred_df = QuantileDiscretizer(numBuckets=10, inputCol=\"pred\", outputCol=\"decile\", relativeError=0.00001,\n",
    "                             handleInvalid=\"error\").fit(df).transform(df)\n",
    "pred_df = pred_df.withColumn('decile', (10 - F.col('decile')).cast('int'))\n",
    "    \n",
    "window_cumsum = Window.orderBy('decile').rangeBetween(Window.unboundedPreceding, 0)\n",
    "total_target = pred_df.select(F.sum(target)).collect()[0][0]\n",
    "df_out = pred_df\\\n",
    "        .groupBy('decile', )\\\n",
    "        .agg(F.count('ln_no').alias('decile_cnt'), F.sum(target).alias('payoff_cnt'))\\\n",
    "        .withColumn('cum_sum', F.sum('payoff_cnt').over(window_cumsum) / total_target)\\\n",
    "        .sort('decile')\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
