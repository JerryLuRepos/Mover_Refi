{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success configuring sparkmagic livy.\n",
      "['https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1', 'https://qlawsdl001038a.ad.datalake.foc.zone:8443/gateway/dsx/livy/v1']\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "from dsx_core_utils import proxy_util,dsxhi_util\n",
    "proxy_util.configure_proxy_livy()\n",
    "dsxhi_util.list_livy_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\"executorCores\": 4, \"numExecutors\": 5, \"executorMemory\": \"10g\", \n",
    " \"driverMemory\": \"8g\", \"proxyUser\": \"jchen-\", \"driverCores\": 1, \n",
    " \"conf\": {\"spark.yarn.appMasterEnv.THEANO_FLAGS\": \"base_compiledir=${PWD}/.theano\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>22879</td><td>application_1590030838276_68046</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://qlawsbidlhm02b.ad.datalake.foc.zone:8090/proxy/application_1590030838276_68046/\">Link</a></td><td><a target=\"_blank\" href=\"https://qlawsbidlhw14b.ad.datalake.foc.zone:8042/node/containerlogs/container_e499_1590030838276_68046_01_000001/aliu-\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s refimodel -k -l python -u https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0.2.6.5.0-292"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "import pyspark\n",
    "import os, sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import length, col, when, count, countDistinct, mean, lit, sum, udf, desc, min, max\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler, VectorSlicer, StringIndexer, QuantileDiscretizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "model_save_path = '/dev/projects/retention_models/refi_payoff/models/'\n",
    "training_data_path = '/dev/projects/retention_models/refi_payoff/training/data/train'\n",
    "test_data_path = '/dev/projects/retention_models/refi_payoff/training/data/test'\n",
    "result_path = '/dev/projects/retention_models/refi_payoff/training/results/'\n",
    "\n",
    "monthly_prep_path = '/dev/projects/retention_models/monthly_snapshot/monthly_preprocessed/'\n",
    "actual_path = '/dev/projects/retention_models/actual_value/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df201612 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201612.parquet').drop('gcid', 'p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201703 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201703.parquet').drop('gcid', 'p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201706 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201706.parquet').drop('gcid', 'p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201709 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201709.parquet').drop('gcid', 'p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201712 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201712.parquet').drop('gcid', 'p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201803 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201803.parquet').drop('gcid', 'p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201806 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201806.parquet').drop('gcid', 'p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201809 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201809.parquet').drop('gcid', 'p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201812 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201812.parquet').drop('p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201903 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201903.parquet').drop('p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201906 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201906.parquet').drop('p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201909 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201909.parquet').drop('p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df201912 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201912.parquet').drop('p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "df202003 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202003.parquet').drop('p_31_50_exp', 'p_married_exp', 'borrowerage_bucket')\n",
    "\n",
    "\n",
    "df_preprocessed = df201612.union(df201703).union(df201706).union(df201709).union(df201712)\\\n",
    "                            .union(df201803).union(df201806).union(df201809).union(df201812)\\\n",
    "                            .union(df201903).union(df201906).union(df201909).union(df201912)\\\n",
    "                            .union(df202003)\\\n",
    "                            .drop('borrowerage_bucket','p_married_exp','p_31_50_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "target = 'refipayoff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Actual Payoff Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+\n",
      "|servicecalendardate|     ln_no|refipayoff|\n",
      "+-------------------+----------+----------+\n",
      "|         2016-10-31|3219834719|       0.0|\n",
      "+-------------------+----------+----------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "actual = spark.read.parquet(actual_path + 'Actual_payoff_20200610.parquet')\\\n",
    "                .select('servicecalendardate', 'ln_no', 'refipayoff')\n",
    "\n",
    "actual.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Join actual data with preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|servicecalendardate|count(ln_no)|sum(refipayoff)|\n",
      "+-------------------+------------+---------------+\n",
      "|         2020-03-31|     1867297|        62522.0|\n",
      "|         2019-12-31|     1802243|       104715.0|\n",
      "|         2019-09-30|     1763391|        95750.0|\n",
      "|         2019-06-30|     1822001|        78836.0|\n",
      "|         2018-12-31|     1726017|        38086.0|\n",
      "|         2018-09-30|     1683583|        27599.0|\n",
      "|         2018-06-30|     1619695|        31482.0|\n",
      "|         2018-03-31|     1577779|        28882.0|\n",
      "|         2017-12-31|     1530004|        29156.0|\n",
      "|         2017-09-30|     1468441|        38311.0|\n",
      "|         2017-06-30|     1423608|        42331.0|\n",
      "|         2017-03-31|     1379094|        31566.0|\n",
      "|         2016-12-31|     1326375|        26235.0|\n",
      "+-------------------+------------+---------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "df_all.groupby('servicecalendardate').agg(count('ln_no'), sum(target)).orderBy(col('servicecalendardate').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# create fundtion for undersampling\n",
    "def data_split(df, train_months, target, rate):\n",
    "    \n",
    "    ## Split Train/Validation From Test ## \n",
    "    train_validate = df.where(col('servicecalendardate').isin(train_months))\n",
    "    \n",
    "    ## Split Out Payoffs into Train/Validate ##\n",
    "    train_refi_all = train_validate.filter(train_validate[target] == '1.')\n",
    "    train_refi, validate_refi = train_refi_all.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "    ## Split Out Non-Payoffs to Train/Validate\n",
    "    train_nonrefi_all = train_validate.filter(train_validate[target] == '0.')\n",
    "    train_nonrefi, validate_nonrefi = train_nonrefi_all.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "    ## Undersample Non-Payoffs ## \n",
    "    train_non, notused1 = train_nonrefi.randomSplit([rate, 1-rate], seed=123)\n",
    "    val_non, notused2 = validate_nonrefi.randomSplit([rate, 1-rate], seed=123)\n",
    "\n",
    "    ## Create Final Training/Validate Sets\n",
    "    train = train_refi.unionAll(train_non)\n",
    "    validate = validate_refi.unionAll(val_non)\n",
    "    train.cache()\n",
    "    \n",
    "    return train, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "train_months = ['2017-12-31', '2018-03-31', '2018-06-30', '2018-09-30', '2018-12-31', '2019-03-31', '2019-06-30']\n",
    "\n",
    "df_train, df_validate = data_split(df_all, train_months, target, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+\n",
      "|servicecalendardate|refipayoff|count|\n",
      "+-------------------+----------+-----+\n",
      "|         2017-12-31|       0.0|52380|\n",
      "|         2017-12-31|       1.0|20430|\n",
      "|         2018-03-31|       0.0|54101|\n",
      "|         2018-03-31|       1.0|20368|\n",
      "|         2018-06-30|       0.0|55490|\n",
      "|         2018-06-30|       1.0|22022|\n",
      "|         2018-09-30|       0.0|58094|\n",
      "|         2018-09-30|       1.0|19246|\n",
      "|         2018-12-31|       0.0|59376|\n",
      "|         2018-12-31|       1.0|26757|\n",
      "|         2019-06-30|       0.0|61442|\n",
      "|         2019-06-30|       1.0|55378|\n",
      "+-------------------+----------+-----+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_train.groupby(['servicecalendardate', target]).count().orderBy('servicecalendardate', target).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creat Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def create_pipeline(target, *arg):\n",
    "    \n",
    "    exclude_cols = ('servicecalendardate', 'ln_no', 'og_note_dt', 'mosaic_group_pur_exp', 'ratespread_min_pur_exp',\n",
    "                'ln_purpose_type', 'investornameshort', 'ln_ann_int_rt', target) \n",
    "    cat_cols = [i[0] for i in df_train.dtypes if ((i[1]=='string') & (~i[0].endswith(exclude_cols)))]\n",
    "    num_cols = [i[0] for i in df_train.dtypes if ((i[1].startswith(('int', 'double'))) & (~i[0].endswith(exclude_cols)))]\n",
    "    \n",
    "    stages = []\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        \n",
    "        #Category indexing with StringIndexer\n",
    "        indexer = StringIndexer(inputCol = col, outputCol = col+'_idx').setHandleInvalid('keep')\n",
    "        stages += [indexer]\n",
    "        \n",
    "    #assemblerInputs = [c+'_vec' for c in cat_cols] + num_cols\n",
    "    assemblerInputs = [c+'_idx' for c in cat_cols] + num_cols\n",
    "    assembler = VectorAssembler(inputCols = assemblerInputs, outputCol = 'vectFeatures')\n",
    "    \n",
    "    stages += [assembler]\n",
    "    \n",
    "    lr = LogisticRegression(maxIter=100, regParam=0.1, elasticNetParam=0.0, fitIntercept = True,\n",
    "                            featuresCol='vectFeatures', labelCol=target)\n",
    "    rf = RandomForestClassifier(numTrees=250, maxDepth = 5, featuresCol='vectFeatures', labelCol=target)\n",
    "    gbt = GBTClassifier(maxIter=100, featuresCol='vectFeatures', labelCol=target)\n",
    "\n",
    "    pipeline_lr = Pipeline(stages = stages + [lr])\n",
    "    pipeline_rf = Pipeline(stages = stages + [rf])\n",
    "    pipeline_gbt = Pipeline(stages = stages + [gbt])\n",
    " \n",
    "    if \"lr\" in arg and \"rf\" in arg and \"gbt\" in arg:\n",
    "        return lr, rf, gbt, pipeline_lr, pipeline_rf, pipeline_gbt\n",
    "    elif \"lr\" in arg and \"rf\" in arg:\n",
    "        return lr, rf, pipeline_lr, pipeline_rf\n",
    "    elif \"lr\" in arg and \"gbt\" in arg:\n",
    "        return lr, gbt, pipeline_lr, pipeline_gbt \n",
    "    elif \"rf\" in arg and \"gbt\" in arg:\n",
    "        return rf, gbt, pipeline_rf, pipeline_gbt\n",
    "    elif \"rf\" in arg:\n",
    "        return rf, pipeline_rf\n",
    "    elif \"lr\" in arg:\n",
    "        return lr, pipeline_lr\n",
    "    elif \"gbt\" in arg:\n",
    "        return gbt, pipeline_gbt\n",
    "    else:\n",
    "        return gbt, pipeline_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fit_pipeline(pipeline, training_dataset):\n",
    "    model_pipeline = pipeline.fit(training_dataset)\n",
    "    return model_pipeline\n",
    "    \n",
    "def persist_modelpersist_(pipeline_model, model_name, model_save_path=model_save_path):\n",
    "    pipeline_model.save(model_save_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "lr, rf, gbt, pipeline_lr, pipeline_rf, pipeline_gbt = create_pipeline(target, \"lr\", \"rf\", \"gbt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Time: 49.82642197608948"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Logistic Regression\n",
    "start = time.time()\n",
    "model_pipeline_lr = fit_pipeline(pipeline_lr, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Logistic Regression Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Time: 61.7143292427063"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Random Forest\n",
    "start = time.time()\n",
    "model_pipeline_rf = fit_pipeline(pipeline_rf, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Random Forest Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Training Time: 723.8272330760956"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Gradient Boosting\n",
    "start = time.time()\n",
    "model_pipeline_gbt = fit_pipeline(pipeline_gbt, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Gradient Boosting Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "persist_modelpersist_(model_pipeline_lr, 'refiV1_4_LR_20200513')\n",
    "persist_modelpersist_(model_pipeline_rf, 'refiV1_4_RF_20200513')\n",
    "persist_modelpersist_(model_pipeline_gbt, 'refiV1_4_GBT_20200513')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "model_pipeline_lr = PipelineModel.load(path = model_save_path+'refiV1_4_LR_20200513')\n",
    "model_pipeline_rf = PipelineModel.load(path = model_save_path+'refiV1_4_RF_20200513')\n",
    "model_pipeline_gbt = PipelineModel.load(path = model_save_path+'refiV1_4_GBT_20200513')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "def extractFeatureImp(model_pipeline, df_train, featuresCol):\n",
    "    \n",
    "    featureImp = model_pipeline.stages[-1].featureImportances\n",
    "    transformed = model_pipeline.transform(df_train)\n",
    "    list_extract = []\n",
    "    \n",
    "    for i in transformed.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + transformed.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "        \n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    \n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "    return varlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx                         name  \\\n",
      "5    13                 ageinmon_exp   \n",
      "7    15              currentcltv_exp   \n",
      "4    12           ratespread_min_exp   \n",
      "6    14                og_mtg_am_exp   \n",
      "8    16                orig_fico_exp   \n",
      "20    6      ln_purpose_type_exp_idx   \n",
      "18    4  loantypedescription_exp_idx   \n",
      "13   21                    ln_tr_exp   \n",
      "1     9               home_value_exp   \n",
      "21    7  og_occupy_stat_type_exp_idx   \n",
      "0     8                   ct_age_exp   \n",
      "19    5     loanamortizationtype_idx   \n",
      "10   18           LiveYears_long_exp   \n",
      "17    3    mosaic_group_refi_exp_idx   \n",
      "16    2                 ct_3_exp_idx   \n",
      "3    11       personnum_per_room_exp   \n",
      "9    17          LiveYears_short_exp   \n",
      "12   20         issingleborrower_exp   \n",
      "15    1                 ct_2_exp_idx   \n",
      "2    10                 p_edu_hs_exp   \n",
      "14    0                 ct_1_exp_idx   \n",
      "11   19            LiveYears_grp_exp   \n",
      "\n",
      "                                                 vals     score  \n",
      "5                                                 NaN  0.148367  \n",
      "7                                                 NaN  0.127979  \n",
      "4                                                 NaN  0.121751  \n",
      "6                                                 NaN  0.107109  \n",
      "8                                                 NaN  0.089590  \n",
      "20     [refinance, cashout refi, purchase, __unknown]  0.061904  \n",
      "18                         [conv, fha, va, __unknown]  0.044631  \n",
      "13                                                NaN  0.043954  \n",
      "1                                                 NaN  0.041655  \n",
      "21  [primary residence, investment property, secon...  0.039836  \n",
      "0                                                 NaN  0.033267  \n",
      "19                            [fixed, arm, __unknown]  0.029034  \n",
      "10                                                NaN  0.027265  \n",
      "17    [other, DEFJK, ACG, BHQ, IL, MNOPRS, __unknown]  0.022627  \n",
      "16                            [0, 1, 2, 3, __unknown]  0.014618  \n",
      "3                                                 NaN  0.010929  \n",
      "9                                                 NaN  0.009944  \n",
      "12                                                NaN  0.007468  \n",
      "15                               [0, 1, 2, __unknown]  0.006982  \n",
      "2                                                 NaN  0.006021  \n",
      "14                                  [0, 1, __unknown]  0.005069  \n",
      "11                                                NaN  0.000000"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# GBT\n",
    "gbt_feature_importance = extractFeatureImp(model_pipeline_gbt, df_train, \"vectFeatures\")\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(gbt_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def predict(model_pipeline, data):\n",
    "    \n",
    "    _scoreUdf = udf(lambda v: float(v[1]), DoubleType())\n",
    "    \n",
    "    prediction = model_pipeline.transform(data)\n",
    "    pred_df = prediction.withColumn('pred', _scoreUdf(prediction['probability']))\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def roc_auc(df, target):   \n",
    "    \n",
    "    pred_df_lr = predict(model_pipeline_lr, df)\n",
    "    pred_df_rf = predict(model_pipeline_rf, df)\n",
    "    pred_df_gbt = predict(model_pipeline_gbt, df)\n",
    "\n",
    "    ### Calculate ROC ###  \n",
    "    evalPred_lr = pred_df_lr.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "    evalPred_rf = pred_df_rf.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "    evalPred_gbt = pred_df_gbt.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "\n",
    "    evaluatorLR = BinaryClassificationEvaluator()\n",
    "    evaluatorRF = BinaryClassificationEvaluator()\n",
    "    evaluatorGBT = BinaryClassificationEvaluator()\n",
    "\n",
    "    print(\"Test Area Under ROC - LR: \" + str(evaluatorLR.evaluate(evalPred_lr, {evaluatorLR.metricName: \"areaUnderROC\"})))        \n",
    "    print(\"Test Area Under ROC - RF: \" + str(evaluatorRF.evaluate(evalPred_rf, {evaluatorRF.metricName: \"areaUnderROC\"})))        \n",
    "    print(\"Test Area Under ROC - GBT: \" + str(evaluatorGBT.evaluate(evalPred_gbt, {evaluatorGBT.metricName: \"areaUnderROC\"})))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC - LR: 0.6332322703577062\n",
      "Test Area Under ROC - RF: 0.680734951367731\n",
      "Test Area Under ROC - GBT: 0.7252698287642437\n",
      "\n",
      "\n",
      "Test Area Under ROC - LR: 0.6343274599427369\n",
      "Test Area Under ROC - RF: 0.6792016999576734\n",
      "Test Area Under ROC - GBT: 0.7196060082250103\n",
      "\n",
      "\n",
      "Test Area Under ROC - LR: 0.6107240826340268\n",
      "Test Area Under ROC - RF: 0.6768353729994416\n",
      "Test Area Under ROC - GBT: 0.708076291895596"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "test_months = ['2017-09-30', '2019-09-30', '2019-12-31']\n",
    "df_test = df_all.where(col('servicecalendardate').isin(test_months))\n",
    "\n",
    "for df in [df_train, df_validate, df_test]:\n",
    "    roc_auc(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "test_months = ['2017-09-30', '2019-09-30', '2019-12-31']\n",
    "df_test = df_all.where(col('servicecalendardate').isin(test_months))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Capture Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def deciles(df, model, target):\n",
    "    \n",
    "    pred_df = predict(model, df)\n",
    "\n",
    "    pred_df = QuantileDiscretizer(numBuckets=10, inputCol=\"pred\", outputCol=\"decile\", relativeError=0.00001,\n",
    "                             handleInvalid=\"error\").fit(pred_df).transform(pred_df)\n",
    "    pred_df = pred_df.withColumn('decile', (10 - F.col('decile')).cast('int'))\n",
    "    \n",
    "    window_cumsum = Window.orderBy('decile').rangeBetween(Window.unboundedPreceding, 0)\n",
    "    total_target = pred_df.select(F.sum(target)).collect()[0][0]\n",
    "    df_out = pred_df\\\n",
    "        .groupBy('decile', )\\\n",
    "        .agg(F.count('ln_no').alias('decile_cnt'), F.sum(target).alias('payoff_cnt'))\\\n",
    "        .withColumn('cum_sum', F.sum('payoff_cnt').over(window_cumsum) / total_target)\\\n",
    "        .sort('decile')\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|servicecalendardate| count|\n",
      "+-------------------+------+\n",
      "|         2018-06-30| 77512|\n",
      "|         2018-09-30| 77340|\n",
      "|         2017-12-31| 72810|\n",
      "|         2018-12-31| 86133|\n",
      "|         2019-06-30|116820|\n",
      "|         2018-03-31| 74469|\n",
      "+-------------------+------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_train.groupBy('servicecalendardate').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "date_label = test_months[2]\n",
    "print(date_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "#print(date_label)\n",
    "#deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_lr, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "#print(date_label)\n",
    "#deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_rf, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    180229|   25866.0|0.24701332187365707|\n",
      "|     2|    180210|   17371.0| 0.4129016855273839|\n",
      "|     3|    180207|   13264.0| 0.5395693071670725|\n",
      "|     4|    180210|   11468.0| 0.6490856133314233|\n",
      "|     5|    180246|    9632.0| 0.7410686148116316|\n",
      "|     6|    180236|    8537.0|  0.822594661700807|\n",
      "|     7|    180205|    7373.0| 0.8930048226137611|\n",
      "|     8|    180240|    5954.0| 0.9498639163443633|\n",
      "|     9|    180230|    3959.0| 0.9876712982858235|\n",
      "|    10|    180230|    1291.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(date_label)\n",
    "deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    180219|   26262.0| 0.2452192425487413|\n",
      "|     2|    180220|   17688.0|0.41037947262269364|\n",
      "|     3|    180210|   13548.0|  0.536882796743109|\n",
      "|     4|    180227|   11711.0| 0.6462332860237544|\n",
      "|     5|    180237|    9875.0| 0.7384402778815269|\n",
      "|     6|    180213|    8778.0| 0.8204041234033017|\n",
      "|     7|    180229|    7604.0| 0.8914058414880107|\n",
      "|     8|    180237|    6172.0| 0.9490363785762307|\n",
      "|     9|    180214|    4091.0| 0.9872357511018152|\n",
      "|    10|    180237|    1367.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "actual1 = spark.read.parquet(actual_path + 'Actual_payoff_20200505.parquet')\\\n",
    "                .select('servicecalendardate', 'ln_no', 'refipayoff')\n",
    "\n",
    "df_all1 = df_preprocessed.join(actual1, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates().where(col('servicecalendardate') == '2019-12-31')\n",
    "\n",
    "deciles(df_all1.filter(col('servicecalendardate') == date_label), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fullMonthPrediction(df):\n",
    "    \n",
    "    pred_df_lr = predict(model_pipeline_lr, df)\n",
    "    validationPredictionsLR = pred_df_lr.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'logRegProb')\n",
    "    \n",
    "    pred_df_rf = predict(model_pipeline_rf, df)\n",
    "    validationPredictionsRF = pred_df_rf.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'randForProb')\n",
    "    \n",
    "    pred_df_gbt = predict(model_pipeline_gbt, df)\n",
    "    validationPredictionsGBT = pred_df_gbt.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'gbtProb')\n",
    "    \n",
    "    combinedFinalPred = validationPredictionsLR.join(validationPredictionsRF, on='ln_no', how='left')\\\n",
    "                                                .join(validationPredictionsGBT, on='ln_no', how='left')\\\n",
    "                                                .dropDuplicates()\n",
    "    \n",
    "    return combinedFinalPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def save_predictions_csv(df_pred, filename, result_path = result_path):\n",
    "    \n",
    "    df_pred.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def pred_save(df, servicedate, filename, result_path = result_path):\n",
    "    \n",
    "    df1 = df.where(col('servicecalendardate') == servicedate)\n",
    "    df_pred = fullMonthPrediction(df1)\n",
    "    \n",
    "    save_predictions_csv(df_pred, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def pred_save_newmonth(preprocessed_file, filename, monthly_prep_path=monthly_prep_path, result_path=result_path):\n",
    "    \n",
    "    df = spark.read.parquet(monthly_prep_path + preprocessed_file)\n",
    "    df_pred = fullMonthPrediction(df)\n",
    "    \n",
    "    print(df.count())\n",
    "    save_predictions_csv(df_pred, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 2017-9\n",
    "pred_save(df_test.where(col('servicecalendardate') == '2017-09-30'), '2017-09-30', 'pred_refiV1_4_sep17_20200514.csv')\n",
    "\n",
    "\n",
    "# 2019-9\n",
    "pred_save(df_test.where(col('servicecalendardate') == '2019-09-30'), '2019-09-30', 'pred_refiV1_4_sep19_20200514.csv')\n",
    "\n",
    "          \n",
    "# 2019-12\n",
    "pred_save(df_test.where(col('servicecalendardate') == '2019-12-31'), '2019-12-31', 'pred_refiV1_4_dec19_20200514.csv')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468441\n",
      "1468441\n",
      "1763391\n",
      "1763391\n",
      "1802243\n",
      "1802243"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "print(df_all.where(col('servicecalendardate') == '2017-09-30').count())\n",
    "print(spark.read.csv(result_path + 'pred_refiV1_4_sep17_20200514.csv', header = True).count())\n",
    "\n",
    "\n",
    "print(df_all.where(col('servicecalendardate') == '2019-09-30').count())\n",
    "print(spark.read.csv(result_path + 'pred_refiV1_4_sep19_20200514.csv', header = True).count())\n",
    "\n",
    "\n",
    "print(df_all.where(col('servicecalendardate') == '2019-12-31').count())\n",
    "print(spark.read.csv(result_path + 'pred_refiV1_4_dec19_20200514.csv', header = True).count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|servicecalendardate|count(ln_no)|sum(refipayoff)|\n",
      "+-------------------+------------+---------------+\n",
      "|         2020-03-31|     1867297|        62522.0|\n",
      "+-------------------+------------+---------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df202003 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202003.parquet')\n",
    "df_preprocessed = df202003.drop('borrowerage_bucket','p_married_exp','p_31_50_exp')\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "df_all.groupby('servicecalendardate').agg(count('ln_no'), sum(target)).orderBy(col('servicecalendardate').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 2020-03\n",
    "pred_save(df_all.where(col('servicecalendardate') == '2020-03-31'), '2020-03-31', 'pred_refiV1_4_mar20_20200514.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867297"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "print(spark.read.csv(result_path + 'pred_refiV1_4_mar20_20200514.csv', header = True).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    186704|   13777.0|0.22035443523879594|\n",
      "|     2|    186751|   10976.0|0.39590864015866417|\n",
      "|     3|    186740|    8277.0| 0.5282940404977448|\n",
      "|     4|    186715|    6727.0| 0.6358881673650875|\n",
      "|     5|    186717|    6492.0| 0.7397236172867151|\n",
      "|     6|    186742|    5759.0| 0.8318351940117079|\n",
      "|     7|    186735|    4741.0| 0.9076645020952625|\n",
      "|     8|    186715|    3187.0|  0.958638559227152|\n",
      "|     9|    186715|    2023.0| 0.9909951697002655|\n",
      "|    10|    186763|     563.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "deciles(df_all.filter(col('servicecalendardate') == '2020-03-31'), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[servicecalendardate: date, count(ln_no): bigint, sum(refipayoff): double]\n",
      "1899099\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    189904|    4044.0|0.23578800069966765|\n",
      "|     2|    189889|    2937.0| 0.4070316599615183|\n",
      "|     3|    189936|    2329.0| 0.5428254912250015|\n",
      "|     4|    189899|    1907.0| 0.6540143431869861|\n",
      "|     5|    189909|    1734.0| 0.7551163197481197|\n",
      "|     6|    189901|    1503.0| 0.8427496938953997|\n",
      "|     7|    189927|    1232.0| 0.9145822401026179|\n",
      "|     8|    189906|     840.0|  0.963558976152994|\n",
      "|     9|    189897|     519.0| 0.9938196023555478|\n",
      "|    10|    189931|     106.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df202004 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202004.parquet')\n",
    "df_preprocessed = df202004.drop('borrowerage_bucket','p_married_exp','p_31_50_exp')\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "\n",
    "pred_save(df_all.where(col('servicecalendardate') == '2020-04-30'), '2020-04-30', 'pred_refiV1_4_apr20_20200514.csv')\n",
    "print(spark.read.csv(result_path + 'pred_refiV1_4_apr20_20200514.csv', header = True).count())\n",
    "\n",
    "deciles(df_all.filter(col('servicecalendardate') == '2020-04-30'), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|servicecalendardate|count(ln_no)|sum(refipayoff)|\n",
      "+-------------------+------------+---------------+\n",
      "|         2020-04-30|     1899099|        17151.0|\n",
      "+-------------------+------------+---------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all.groupby('servicecalendardate').agg(count('ln_no'), sum(target)).orderBy(col('servicecalendardate').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    184673|   24445.0|0.23387644588168885|\n",
      "|     2|    184677|   17463.0| 0.4009529185522527|\n",
      "|     3|    184698|   13762.0| 0.5326202389950345|\n",
      "|     4|    184681|   12389.0| 0.6511514432506387|\n",
      "|     5|    184695|   10712.0| 0.7536380248945188|\n",
      "|     6|    184689|    8754.0| 0.8373915289750385|\n",
      "|     7|    184672|    7048.0|  0.904822954238861|\n",
      "|     8|    184709|    5449.0| 0.9569560184077841|\n",
      "|     9|    184661|    3506.0| 0.9904995168435051|\n",
      "|    10|    184709|     993.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df202002 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202002.parquet')\n",
    "df_preprocessed = df202002.drop('borrowerage_bucket','p_married_exp','p_31_50_exp')\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "pred_save(df_all.where(col('servicecalendardate') == '2020-02-29'), '2020-02-29', 'pred_refiV1_4_feb20_20200514.csv')\n",
    "\n",
    "deciles(df_all.filter(col('servicecalendardate') == '2020-02-29'), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ln_no: string (nullable = true)\n",
      " |-- og_note_dt: string (nullable = true)\n",
      " |-- loantypedescription: string (nullable = true)\n",
      " |-- loanamortizationtype: string (nullable = true)\n",
      " |-- investornameshort: string (nullable = true)\n",
      " |-- loan_term: string (nullable = true)\n",
      " |-- loan_age: string (nullable = true)\n",
      " |-- currentcltv: string (nullable = true)\n",
      " |-- loan_interest_rate: string (nullable = true)\n",
      " |-- logRegProb: string (nullable = true)\n",
      " |-- randForProb: string (nullable = true)\n",
      " |-- gbtProb: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all = spark.read.csv(result_path + 'pred_refiV1_3_feb20_20200305.csv', header = True)\n",
    "df_all.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    184678|   21641.0|0.20704930109738712|\n",
      "|     2|    184685|   16132.0|0.36139149070521714|\n",
      "|     3|    184684|   13726.0|0.49271438275561846|\n",
      "|     4|    184685|   12516.0| 0.6124606538398982|\n",
      "|     5|    184675|   10899.0| 0.7167363496330881|\n",
      "|     6|    184713|    9263.0|  0.805359688483654|\n",
      "|     7|    184651|    8106.0| 0.8829134815013251|\n",
      "|     8|    184714|    6831.0| 0.9482687689555209|\n",
      "|     9|    184665|    4287.0| 0.9892844500148296|\n",
      "|    10|    184714|    1120.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df = df_all.withColumn('pred', col('gbtProb').cast('double'))\\\n",
    "                .withColumn('servicecalendardate', lit('2020-02-29'))\\\n",
    "                .join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()\n",
    "pred_df = QuantileDiscretizer(numBuckets=10, inputCol=\"pred\", outputCol=\"decile\", relativeError=0.00001,\n",
    "                             handleInvalid=\"error\").fit(df).transform(df)\n",
    "pred_df = pred_df.withColumn('decile', (10 - F.col('decile')).cast('int'))\n",
    "    \n",
    "window_cumsum = Window.orderBy('decile').rangeBetween(Window.unboundedPreceding, 0)\n",
    "total_target = pred_df.select(F.sum(target)).collect()[0][0]\n",
    "df_out = pred_df\\\n",
    "        .groupBy('decile', )\\\n",
    "        .agg(F.count('ln_no').alias('decile_cnt'), F.sum(target).alias('payoff_cnt'))\\\n",
    "        .withColumn('cum_sum', F.sum('payoff_cnt').over(window_cumsum) / total_target)\\\n",
    "        .sort('decile')\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
