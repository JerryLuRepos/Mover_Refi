{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success configuring sparkmagic livy.\n",
      "['https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1']\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "from dsx_core_utils import proxy_util,dsxhi_util\n",
    "proxy_util.configure_proxy_livy() \n",
    "dsxhi_util.list_livy_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\"executorCores\": 2, \"numExecutors\": 10, \"executorMemory\": \"15g\", \n",
    " \"driverMemory\": \"12g\", \"proxyUser\": \"aliu-\", \"driverCores\": 1, \n",
    " \"conf\": {\"spark.yarn.appMasterEnv.THEANO_FLAGS\": \"base_compiledir=${PWD}/.theano\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>20985</td><td>application_1582863878751_121765</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://qlawsbidlhm02b.ad.datalake.foc.zone:8090/proxy/application_1582863878751_121765/\">Link</a></td><td><a target=\"_blank\" href=\"https://qlawsbidlhw14a.ad.datalake.foc.zone:8042/node/containerlogs/container_e498_1582863878751_121765_01_000001/aliu-\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s inactivePurchaseLeadGen -k -l python -u https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "import pyspark\n",
    "import os, sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lower\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import last_day, rand, dense_rank, last_day, col, size, length, when, upper, unix_timestamp, avg, substring, lower, udf, sum, count, lit, mean, concat, countDistinct, desc, from_unixtime, row_number, year, month, to_date, upper, months_between\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, QuantileDiscretizer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from os.path import expanduser, join, abspath\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "model_save_path = '/dev/projects/retention_models/purchase_payoff/models/'\n",
    "training_data_path = '/dev/projects/retention_models/purchase_payoff/training/data/train/'\n",
    "test_data_path = '/dev/projects/retention_models/purchase_payoff/training/data/test/'\n",
    "result_path = '/dev/projects/retention_models/purchase_payoff/training/results/mover_12/'\n",
    "\n",
    "monthly_prep_path = '/dev/projects/retention_models/monthly_snapshot/monthly_preprocessed/'\n",
    "actual_path = '/dev/projects/retention_models/actual_value/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Load Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data for Mover Purchase V1.1 6-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df201612 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201612.parquet').drop('gcid')\n",
    "df201703 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201703.parquet').drop('gcid')\n",
    "df201706 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201706.parquet').drop('gcid')\n",
    "df201709 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201709.parquet').drop('gcid')\n",
    "df201712 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201712.parquet').drop('gcid')\n",
    "df201803 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201803.parquet').drop('gcid')\n",
    "df201806 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201806.parquet').drop('gcid')\n",
    "df201809 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201809.parquet').drop('gcid')\n",
    "df201812 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201812.parquet')\n",
    "df201903 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201903.parquet')\n",
    "df201906 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201906.parquet')\n",
    "df201909 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201909.parquet')\n",
    "df201912 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201912.parquet')\n",
    "df202003 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202003.parquet')\n",
    "\n",
    "\n",
    "df_preprocessed = df201612.union(df201703).union(df201706).union(df201709).union(df201712)\\\n",
    "                            .union(df201803).union(df201806).union(df201809).union(df201812)\\\n",
    "                            .union(df201903).union(df201906).union(df201909).union(df201912)\\\n",
    "                            .union(df202003)\\\n",
    "                            .drop('borrowerage_bucket','p_married_exp','p_31_50_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------------------+\n",
      "|servicecalendardate|     ln_no|purchasepayoff_6_12|\n",
      "+-------------------+----------+-------------------+\n",
      "|         2016-10-31|1232492399|                0.0|\n",
      "+-------------------+----------+-------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "actual = spark.read.parquet(actual_path + 'Actual_payoff_20200505.parquet')\\\n",
    "                .select('servicecalendardate', 'ln_no', 'purchasepayoff_6_12')\n",
    "\n",
    "actual.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "target = 'purchasepayoff_6_12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+------------------------+\n",
      "|servicecalendardate|count(ln_no)|sum(purchasepayoff_6_12)|\n",
      "+-------------------+------------+------------------------+\n",
      "|         2020-03-31|     1867297|                     0.0|\n",
      "|         2019-12-31|     1802243|                     0.0|\n",
      "|         2019-09-30|     1763391|                    42.0|\n",
      "|         2019-06-30|     1822001|                 10724.0|\n",
      "|         2018-12-31|     1726017|                 31292.0|\n",
      "|         2018-09-30|     1683583|                 39172.0|\n",
      "|         2018-06-30|     1619695|                 33992.0|\n",
      "|         2018-03-31|     1577779|                 24945.0|\n",
      "|         2017-12-31|     1530004|                 27817.0|\n",
      "|         2017-09-30|     1468441|                 34156.0|\n",
      "|         2017-06-30|     1423608|                 29665.0|\n",
      "|         2017-03-31|     1379094|                 22382.0|\n",
      "|         2016-12-31|     1326375|                 24884.0|\n",
      "+-------------------+------------+------------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all.groupby('servicecalendardate').agg(count('ln_no'), sum(target)).orderBy(col('servicecalendardate').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Under-sample negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# create fundtion for undersampling\n",
    "def data_split(df, train_months, target, rate):\n",
    "    \n",
    "    ## Split Train/Validation From Test ## \n",
    "    train_validate = df.where(col('servicecalendardate').isin(train_months))\n",
    "    \n",
    "    ## Split Out Payoffs into Train/Validate ##\n",
    "    train_refi_all = train_validate.filter(train_validate[target] == '1.')\n",
    "    train_refi, validate_refi = train_refi_all.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "    ## Split Out Non-Payoffs to Train/Validate\n",
    "    train_nonrefi_all = train_validate.filter(train_validate[target] == '0.')\n",
    "    train_nonrefi, validate_nonrefi = train_nonrefi_all.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "    ## Undersample Non-Payoffs ## \n",
    "    train_non, notused1 = train_nonrefi.randomSplit([rate, 1-rate], seed=123)\n",
    "    val_non, notused2 = validate_nonrefi.randomSplit([rate, 1-rate], seed=123)\n",
    "\n",
    "    ## Create Final Training/Validate Sets\n",
    "    train = train_refi.unionAll(train_non)\n",
    "    validate = validate_refi.unionAll(val_non)\n",
    "    train.cache()\n",
    "    \n",
    "    return train, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "train_months = ['2017-06-30', '2017-09-30', '2017-12-31', '2018-03-31', '2018-06-30', '2018-09-30']\n",
    "\n",
    "df_train, df_validate = data_split(df_all, train_months, target, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+\n",
      "|servicecalendardate|purchasepayoff_6_12|count|\n",
      "+-------------------+-------------------+-----+\n",
      "|         2017-06-30|                0.0|48641|\n",
      "|         2017-06-30|                1.0|20803|\n",
      "|         2017-09-30|                0.0|50061|\n",
      "|         2017-09-30|                1.0|24034|\n",
      "|         2017-12-31|                0.0|52466|\n",
      "|         2017-12-31|                1.0|19483|\n",
      "|         2018-03-31|                0.0|54495|\n",
      "|         2018-03-31|                1.0|17388|\n",
      "|         2018-06-30|                0.0|55814|\n",
      "|         2018-06-30|                1.0|23798|\n",
      "|         2018-09-30|                0.0|57806|\n",
      "|         2018-09-30|                1.0|27639|\n",
      "+-------------------+-------------------+-----+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_train.groupby(['servicecalendardate', target]).count().orderBy('servicecalendardate', target).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def create_pipeline(target, *arg):\n",
    "    \n",
    "    exclude_cols = ('servicecalendardate', 'ln_no', 'og_note_dt', 'mosaic_group_pur_exp', 'ratespread_min_pur_exp',\n",
    "                'ln_purpose_type', 'investornameshort', 'ln_ann_int_rt', target) \n",
    "    cat_cols = [i[0] for i in df_train.dtypes if ((i[1]=='string') & (~i[0].endswith(exclude_cols)))]\n",
    "    num_cols = [i[0] for i in df_train.dtypes if ((i[1].startswith(('int', 'double'))) & (~i[0].endswith(exclude_cols)))]\n",
    "    \n",
    "    stages = []\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        \n",
    "        #Category indexing with StringIndexer\n",
    "        indexer = StringIndexer(inputCol = col, outputCol = col+'_idx').setHandleInvalid('keep')\n",
    "        stages += [indexer]\n",
    "        \n",
    "    #assemblerInputs = [c+'_vec' for c in cat_cols] + num_cols\n",
    "    assemblerInputs = [c+'_idx' for c in cat_cols] + num_cols\n",
    "    assembler = VectorAssembler(inputCols = assemblerInputs, outputCol = 'vectFeatures')\n",
    "    \n",
    "    stages += [assembler]\n",
    "    \n",
    "    lr = LogisticRegression(maxIter=100, regParam=0.1, elasticNetParam=0.0, fitIntercept = True,\n",
    "                            featuresCol='vectFeatures', labelCol=target)\n",
    "    rf = RandomForestClassifier(numTrees=250, maxDepth = 5, featuresCol='vectFeatures', labelCol=target)\n",
    "    gbt = GBTClassifier(maxIter=100, featuresCol='vectFeatures', labelCol=target)\n",
    "\n",
    "    pipeline_lr = Pipeline(stages = stages + [lr])\n",
    "    pipeline_rf = Pipeline(stages = stages + [rf])\n",
    "    pipeline_gbt = Pipeline(stages = stages + [gbt])\n",
    " \n",
    "    if \"lr\" in arg and \"rf\" in arg and \"gbt\" in arg:\n",
    "        return lr, rf, gbt, pipeline_lr, pipeline_rf, pipeline_gbt\n",
    "    elif \"lr\" in arg and \"rf\" in arg:\n",
    "        return lr, rf, pipeline_lr, pipeline_rf\n",
    "    elif \"lr\" in arg and \"gbt\" in arg:\n",
    "        return lr, gbt, pipeline_lr, pipeline_gbt \n",
    "    elif \"rf\" in arg and \"gbt\" in arg:\n",
    "        return rf, gbt, pipeline_rf, pipeline_gbt\n",
    "    elif \"rf\" in arg:\n",
    "        return rf, pipeline_rf\n",
    "    elif \"lr\" in arg:\n",
    "        return lr, pipeline_lr\n",
    "    elif \"gbt\" in arg:\n",
    "        return gbt, pipeline_gbt\n",
    "    else:\n",
    "        return gbt, pipeline_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fit_pipeline(pipeline, training_dataset):\n",
    "    model_pipeline = pipeline.fit(training_dataset)\n",
    "    return model_pipeline\n",
    "    \n",
    "def persist_modelpersist_(pipeline_model, model_name, model_save_path=model_save_path):\n",
    "    pipeline_model.save(model_save_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "lr, rf, gbt, pipeline_lr, pipeline_rf, pipeline_gbt = create_pipeline(target, \"lr\", \"rf\", \"gbt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Time: 226.72631573677063"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Logistic Regression\n",
    "start = time.time()\n",
    "model_pipeline_lr = fit_pipeline(pipeline_lr, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Logistic Regression Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Time: 281.80665040016174"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Random Forest\n",
    "start = time.time()\n",
    "model_pipeline_rf = fit_pipeline(pipeline_rf, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Random Forest Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Training Time: 3686.442395925522"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Gradient Boosting\n",
    "start = time.time()\n",
    "model_pipeline_gbt = fit_pipeline(pipeline_gbt, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Gradient Boosting Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "persist_modelpersist_(model_pipeline_lr, 'Mover_6to12_V1_1_LR_20200516')\n",
    "persist_modelpersist_(model_pipeline_rf, 'Mover_6to12_V1_1_RF_20200516')\n",
    "persist_modelpersist_(model_pipeline_gbt, 'Mover_6to12_V1_1_GBT_20200516')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "model_pipeline_lr = PipelineModel.load(path = model_save_path+'Mover_6to12_V1_1_LR_20200516')\n",
    "model_pipeline_rf = PipelineModel.load(path = model_save_path+'Mover_6to12_V1_1_RF_20200516')\n",
    "model_pipeline_gbt = PipelineModel.load(path = model_save_path+'Mover_6to12_V1_1_GBT_20200516')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "def extractFeatureImp(model_pipeline, df_train, featuresCol):\n",
    "    \n",
    "    featureImp = model_pipeline.stages[-1].featureImportances\n",
    "    transformed = model_pipeline.transform(df_train)\n",
    "    list_extract = []\n",
    "    \n",
    "    for i in transformed.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + transformed.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "        \n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    \n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "    return varlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx                         name  \\\n",
      "5    13                 ageinmon_exp   \n",
      "7    15              currentcltv_exp   \n",
      "10   18           LiveYears_long_exp   \n",
      "6    14                og_mtg_am_exp   \n",
      "8    16                orig_fico_exp   \n",
      "4    12           ratespread_min_exp   \n",
      "1     9               home_value_exp   \n",
      "17    3    mosaic_group_refi_exp_idx   \n",
      "20    6      ln_purpose_type_exp_idx   \n",
      "13   21                    ln_tr_exp   \n",
      "0     8                   ct_age_exp   \n",
      "3    11       personnum_per_room_exp   \n",
      "21    7  og_occupy_stat_type_exp_idx   \n",
      "9    17          LiveYears_short_exp   \n",
      "19    5     loanamortizationtype_idx   \n",
      "2    10                 p_edu_hs_exp   \n",
      "18    4  loantypedescription_exp_idx   \n",
      "16    2                 ct_3_exp_idx   \n",
      "12   20         issingleborrower_exp   \n",
      "11   19            LiveYears_grp_exp   \n",
      "15    1                 ct_2_exp_idx   \n",
      "14    0                 ct_1_exp_idx   \n",
      "\n",
      "                                                 vals     score  \n",
      "5                                                 NaN  0.101026  \n",
      "7                                                 NaN  0.099307  \n",
      "10                                                NaN  0.089132  \n",
      "6                                                 NaN  0.086810  \n",
      "8                                                 NaN  0.086006  \n",
      "4                                                 NaN  0.077951  \n",
      "1                                                 NaN  0.065928  \n",
      "17    [DEFJK, other, ACG, BHQ, IL, MNOPRS, __unknown]  0.057982  \n",
      "20     [refinance, cashout refi, purchase, __unknown]  0.044597  \n",
      "13                                                NaN  0.041639  \n",
      "0                                                 NaN  0.039544  \n",
      "3                                                 NaN  0.037583  \n",
      "21  [primary residence, investment property, secon...  0.034221  \n",
      "9                                                 NaN  0.027778  \n",
      "19                            [fixed, arm, __unknown]  0.027103  \n",
      "2                                                 NaN  0.025538  \n",
      "18                         [conv, fha, va, __unknown]  0.024679  \n",
      "16                            [0, 1, 2, 3, __unknown]  0.011137  \n",
      "12                                                NaN  0.010968  \n",
      "11                                                NaN  0.006026  \n",
      "15                               [0, 1, 2, __unknown]  0.003658  \n",
      "14                                  [0, 1, __unknown]  0.001386"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "gbt_feature_importance = extractFeatureImp(model_pipeline_gbt, df_train, \"vectFeatures\")\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(gbt_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def predict(model_pipeline, data):\n",
    "    \n",
    "    _scoreUdf = udf(lambda v: float(v[1]), DoubleType())\n",
    "    \n",
    "    prediction = model_pipeline.transform(data)\n",
    "    pred_df = prediction.withColumn('pred', _scoreUdf(prediction['probability']))\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def roc_auc(df, target):   \n",
    "    \n",
    "    pred_df_lr = predict(model_pipeline_lr, df)\n",
    "    pred_df_rf = predict(model_pipeline_rf, df)\n",
    "    pred_df_gbt = predict(model_pipeline_gbt, df)\n",
    "\n",
    "    ### Calculate ROC ###  \n",
    "    evalPred_lr = pred_df_lr.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "    evalPred_rf = pred_df_rf.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "    evalPred_gbt = pred_df_gbt.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "\n",
    "    evaluatorLR = BinaryClassificationEvaluator()\n",
    "    evaluatorRF = BinaryClassificationEvaluator()\n",
    "    evaluatorGBT = BinaryClassificationEvaluator()\n",
    "\n",
    "    print(\"Test Area Under ROC - LR: \" + str(evaluatorLR.evaluate(evalPred_lr, {evaluatorLR.metricName: \"areaUnderROC\"})))        \n",
    "    print(\"Test Area Under ROC - RF: \" + str(evaluatorRF.evaluate(evalPred_rf, {evaluatorRF.metricName: \"areaUnderROC\"})))        \n",
    "    print(\"Test Area Under ROC - GBT: \" + str(evaluatorGBT.evaluate(evalPred_gbt, {evaluatorGBT.metricName: \"areaUnderROC\"})))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC - LR: 0.6033404953545035\n",
      "Test Area Under ROC - RF: 0.6223009772948884\n",
      "Test Area Under ROC - GBT: 0.6570597923923083\n",
      "\n",
      "\n",
      "Test Area Under ROC - LR: 0.604224026454145\n",
      "Test Area Under ROC - RF: 0.6216724988467393\n",
      "Test Area Under ROC - GBT: 0.6447669991316481\n",
      "\n",
      "\n",
      "Test Area Under ROC - LR: 0.5919226796607918\n",
      "Test Area Under ROC - RF: 0.6087913550884938\n",
      "Test Area Under ROC - GBT: 0.6295103473346155"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "test_months = ['2016-12-31', '2017-03-31', '2018-12-31']\n",
    "df_test = df_all.where(col('servicecalendardate').isin(test_months))\n",
    "\n",
    "for df in [df_train, df_validate, df_test]:\n",
    "    roc_auc(df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Capture Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def deciles(df, model, target):\n",
    "    \n",
    "    pred_df = predict(model, df)\n",
    "\n",
    "    pred_df = QuantileDiscretizer(numBuckets=10, inputCol=\"pred\", outputCol=\"decile\", relativeError=0.00001,\n",
    "                             handleInvalid=\"error\").fit(pred_df).transform(pred_df)\n",
    "    pred_df = pred_df.withColumn('decile', (10 - F.col('decile')).cast('int'))\n",
    "    \n",
    "    window_cumsum = Window.orderBy('decile').rangeBetween(Window.unboundedPreceding, 0)\n",
    "    total_target = pred_df.select(F.sum(target)).collect()[0][0]\n",
    "    df_out = pred_df\\\n",
    "        .groupBy('decile', )\\\n",
    "        .agg(F.count('ln_no').alias('decile_cnt'), F.sum(target).alias('payoff_cnt'))\\\n",
    "        .withColumn('cum_sum', F.sum('payoff_cnt').over(window_cumsum) / total_target)\\\n",
    "        .sort('decile')\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-31"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "date_label = test_months[2]\n",
    "print(date_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-31\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    172608|    4439.0|0.14185734373002684|\n",
      "|     2|    172596|    4043.0| 0.2710596957688866|\n",
      "|     3|    172591|    3668.0|0.38827815416080785|\n",
      "|     4|    172559|    3435.0|0.49805061996676464|\n",
      "|     5|    172645|    3441.0| 0.6080148280710724|\n",
      "|     6|    172605|    3287.0| 0.7130576505177042|\n",
      "|     7|    172606|    2894.0| 0.8055413524223444|\n",
      "|     8|    172592|    2450.0| 0.8838361242490094|\n",
      "|     9|    172609|    2053.0| 0.9494439473347821|\n",
      "|    10|    172606|    1582.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(date_label)\n",
    "deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_lr, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-31\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    172604|    4848.0|0.15492777706762112|\n",
      "|     2|    172595|    4113.0|0.28636712258724273|\n",
      "|     3|    172593|    3589.0| 0.4010609740508756|\n",
      "|     4|    172724|    3643.0| 0.5174805061996677|\n",
      "|     5|    172468|    3650.0| 0.6341237376965358|\n",
      "|     6|    172589|    3022.0| 0.7306979419659977|\n",
      "|     7|    172612|    2617.0| 0.8143295410967659|\n",
      "|     8|    172629|    2633.0| 0.8984724530231369|\n",
      "|     9|    172587|    2030.0| 0.9633452639652307|\n",
      "|    10|    172616|    1147.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(date_label)\n",
    "deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_rf, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-31\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    172565|    5277.0|0.16863735139971878|\n",
      "|     2|    172619|    4302.0|0.30611657931739744|\n",
      "|     3|    172609|    4084.0| 0.4366291703949891|\n",
      "|     4|    172628|    3596.0| 0.5515467212066982|\n",
      "|     5|    172580|    3340.0| 0.6582832672887639|\n",
      "|     6|    172602|    2970.0|  0.753195704972517|\n",
      "|     7|    172604|    2692.0| 0.8392240828326729|\n",
      "|     8|    172606|    2276.0| 0.9119583280071584|\n",
      "|     9|    172605|    1825.0| 0.9702799437555925|\n",
      "|    10|    172599|     930.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(date_label)\n",
    "deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fullMonthPrediction(df):\n",
    "    \n",
    "    pred_df_lr = predict(model_pipeline_lr, df)\n",
    "    validationPredictionsLR = pred_df_lr.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'logRegProb')\n",
    "    \n",
    "    pred_df_rf = predict(model_pipeline_rf, df)\n",
    "    validationPredictionsRF = pred_df_rf.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'randForProb')\n",
    "    \n",
    "    pred_df_gbt = predict(model_pipeline_gbt, df)\n",
    "    validationPredictionsGBT = pred_df_gbt.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'gbtProb')\n",
    "    \n",
    "    combinedFinalPred = validationPredictionsLR.join(validationPredictionsRF, on='ln_no', how='left')\\\n",
    "                                                .join(validationPredictionsGBT, on='ln_no', how='left')\\\n",
    "                                                .dropDuplicates()\n",
    "    \n",
    "    return combinedFinalPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def pred_save(df, servicedate, filename, result_path = result_path):\n",
    "    \n",
    "    df1 = df.where(col('servicecalendardate') == servicedate)\n",
    "    df_pred = fullMonthPrediction(df1)\n",
    "    \n",
    "    df_pred.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def pred_save_newmonth(preprocessed_file, filename, monthly_prep_path=monthly_prep_path, result_path=result_path):\n",
    "    \n",
    "    df = spark.read.parquet(monthly_prep_path + preprocessed_file)\n",
    "    df_pred = fullMonthPrediction(df)\n",
    "    \n",
    "    print(df.count())\n",
    "    df_pred.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 2017-6\n",
    "pred_save(df_all, '2017-06-30', 'pred_MoverV1_1_jun17_20200514.csv')\n",
    "\n",
    "# 2017-9\n",
    "pred_save(df_all, '2017-09-30', 'pred_MoverV1_1_sep17_20200514.csv')\n",
    "\n",
    "# 2017-12\n",
    "pred_save(df_all, '2017-12-31', 'pred_MoverV1_1_dec17_20200514.csv')\n",
    "\n",
    "# 2018-3\n",
    "pred_save(df_all, '2018-03-31', 'pred_MoverV1_1_mar18_20200514.csv')\n",
    "\n",
    "# 2018-6\n",
    "pred_save(df_all, '2018-06-30', 'pred_MoverV1_1_jun18_20200514.csv')\n",
    "\n",
    "# 2018-9\n",
    "pred_save(df_all, '2018-09-30', 'pred_MoverV1_1_sep18_20200514.csv')\n",
    "\n",
    "# 2018-12\n",
    "pred_save(df_all, '2018-12-31', 'pred_MoverV1_1_dec18_20200514.csv')\n",
    "\n",
    "# 2019-03\n",
    "pred_save(df_all, '2019-03-31', 'pred_MoverV1_1_mar19_20200514.csv')\n",
    "\n",
    "# 2019-06\n",
    "pred_save(df_all, '2019-06-30', 'pred_MoverV1_1_jun19_20200514.csv')\n",
    "\n",
    "# 2019-09\n",
    "pred_save(df_all, '2019-09-30', 'pred_MoverV1_1_sep19_20200514.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 2018-1\n",
    "pred_save_newmonth('monthly_preprocessed_201801.parquet', 'pred_MoverV3_2_jan18_20190722.csv')\n",
    "\n",
    "# 2018-2\n",
    "pred_save_newmonth('monthly_preprocessed_201802.parquet', 'pred_MoverV3_2_feb18_20190722.csv')\n",
    "\n",
    "# 2018-4\n",
    "pred_save_newmonth('monthly_preprocessed_201804.parquet', 'pred_MoverV3_2_apr18_20190722.csv')\n",
    "\n",
    "# 2018-5\n",
    "pred_save_newmonth('monthly_preprocessed_201805.parquet', 'pred_MoverV3_2_may18_20190722.csv')\n",
    "\n",
    "# 2018-7\n",
    "pred_save_newmonth('monthly_preprocessed_201807.parquet', 'pred_MoverV3_2_jul18_20190722.csv')\n",
    "\n",
    "# 2018-8\n",
    "pred_save_newmonth('monthly_preprocessed_201808.parquet', 'pred_MoverV3_2_aug18_20190722.csv')\n",
    "\n",
    "# 2018-10\n",
    "pred_save_newmonth('monthly_preprocessed_201810.parquet', 'pred_MoverV3_2_oct18_20190722.csv')\n",
    "\n",
    "# 2018-11\n",
    "pred_save_newmonth('monthly_preprocessed_201811.parquet', 'pred_MoverV3_2_nov18_20190722.csv')\n",
    "\n",
    "# 2019-1\n",
    "pred_save_newmonth('monthly_preprocessed_201901.parquet', 'pred_MoverV3_2_jan19_20190722.csv')\n",
    "\n",
    "# 2019-2\n",
    "pred_save_newmonth('monthly_preprocessed_201902.parquet', 'pred_MoverV3_2_feb19_20190722.csv')\n",
    "\n",
    "# 2019-4\n",
    "pred_save_newmonth('monthly_preprocessed_201904.parquet', 'pred_MoverV3_2_apr19_20190722.csv')\n",
    "\n",
    "# 2019-5\n",
    "pred_save_newmonth('monthly_preprocessed_201905.parquet', 'pred_MoverV3_2_may19_20190722.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
