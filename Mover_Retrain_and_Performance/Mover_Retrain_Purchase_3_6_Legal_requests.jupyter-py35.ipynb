{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success configuring sparkmagic livy.\n",
      "['https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1']\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "from dsx_core_utils import proxy_util,dsxhi_util\n",
    "proxy_util.configure_proxy_livy() \n",
    "dsxhi_util.list_livy_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\"executorCores\": 2, \"numExecutors\": 10, \"executorMemory\": \"15g\", \n",
    " \"driverMemory\": \"12g\", \"proxyUser\": \"aliu-\", \"driverCores\": 1, \n",
    " \"conf\": {\"spark.yarn.appMasterEnv.THEANO_FLAGS\": \"base_compiledir=${PWD}/.theano\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>20924</td><td>application_1582863878751_118416</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://qlawsbidlhm02b.ad.datalake.foc.zone:8090/proxy/application_1582863878751_118416/\">Link</a></td><td><a target=\"_blank\" href=\"https://qlawsbidlhw14a.ad.datalake.foc.zone:8042/node/containerlogs/container_e498_1582863878751_118416_01_000001/aliu-\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s inactivePurchaseLeadGen -k -l python -u https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "import pyspark\n",
    "import os, sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lower\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import last_day, rand, dense_rank, last_day, col, size, length, when, upper, unix_timestamp, avg, substring, lower, udf, sum, count, lit, mean, concat, countDistinct, desc, from_unixtime, row_number, year, month, to_date, upper, months_between\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, QuantileDiscretizer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from os.path import expanduser, join, abspath\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "model_save_path = '/dev/projects/retention_models/purchase_payoff/models/'\n",
    "training_data_path = '/dev/projects/retention_models/purchase_payoff/training/data/train/'\n",
    "test_data_path = '/dev/projects/retention_models/purchase_payoff/training/data/test/'\n",
    "result_path = '/dev/projects/retention_models/purchase_payoff/training/results/mover_6/'\n",
    "\n",
    "monthly_prep_path = '/dev/projects/retention_models/monthly_snapshot/monthly_preprocessed/'\n",
    "actual_path = '/dev/projects/retention_models/actual_value/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Load Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data for Mover Purchase V1.1 3-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df201612 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201612.parquet').drop('gcid')\n",
    "df201703 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201703.parquet').drop('gcid')\n",
    "df201706 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201706.parquet').drop('gcid')\n",
    "df201709 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201709.parquet').drop('gcid')\n",
    "df201712 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201712.parquet').drop('gcid')\n",
    "df201803 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201803.parquet').drop('gcid')\n",
    "df201806 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201806.parquet').drop('gcid')\n",
    "df201809 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201809.parquet').drop('gcid')\n",
    "df201812 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201812.parquet')\n",
    "df201903 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201903.parquet')\n",
    "df201906 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201906.parquet')\n",
    "df201909 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201909.parquet')\n",
    "df201912 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201912.parquet')\n",
    "df202003 = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_202003.parquet')\n",
    "\n",
    "\n",
    "df_preprocessed = df201612.union(df201703).union(df201706).union(df201709).union(df201712)\\\n",
    "                            .union(df201803).union(df201806).union(df201809).union(df201812)\\\n",
    "                            .union(df201903).union(df201906).union(df201909).union(df201912)\\\n",
    "                            .union(df202003)\\\n",
    "                            .drop('borrowerage_bucket','p_married_exp','p_31_50_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- servicecalendardate: date (nullable = true)\n",
      " |-- ln_no: string (nullable = true)\n",
      " |-- og_note_dt: timestamp (nullable = true)\n",
      " |-- ct_age_exp: integer (nullable = true)\n",
      " |-- ct_1_exp: string (nullable = true)\n",
      " |-- ct_2_exp: string (nullable = true)\n",
      " |-- ct_3_exp: string (nullable = true)\n",
      " |-- home_value_exp: double (nullable = true)\n",
      " |-- p_edu_hs_exp: integer (nullable = true)\n",
      " |-- personnum_per_room_exp: double (nullable = true)\n",
      " |-- mosaic_group_refi_exp: string (nullable = true)\n",
      " |-- mosaic_group_pur_exp: string (nullable = true)\n",
      " |-- ratespread_min_exp: double (nullable = true)\n",
      " |-- ratespread_min_pur_exp: double (nullable = true)\n",
      " |-- ln_ann_int_rt: double (nullable = true)\n",
      " |-- loantypedescription_exp: string (nullable = true)\n",
      " |-- loanamortizationtype: string (nullable = true)\n",
      " |-- ageinmon_exp: double (nullable = true)\n",
      " |-- og_mtg_am_exp: double (nullable = true)\n",
      " |-- currentcltv_exp: double (nullable = true)\n",
      " |-- orig_fico_exp: integer (nullable = true)\n",
      " |-- LiveYears_short_exp: double (nullable = true)\n",
      " |-- LiveYears_long_exp: double (nullable = true)\n",
      " |-- LiveYears_grp_exp: double (nullable = true)\n",
      " |-- ln_purpose_type_exp: string (nullable = true)\n",
      " |-- ln_purpose_type: string (nullable = true)\n",
      " |-- og_occupy_stat_type_exp: string (nullable = true)\n",
      " |-- issingleborrower_exp: integer (nullable = true)\n",
      " |-- ln_tr_exp: integer (nullable = true)\n",
      " |-- investornameshort: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_preprocessed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Actual Payoff Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------------------+\n",
      "|servicecalendardate|     ln_no|purchasepayoff_3_6|\n",
      "+-------------------+----------+------------------+\n",
      "|         2016-10-31|1232492399|               0.0|\n",
      "+-------------------+----------+------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "actual = spark.read.parquet(actual_path + 'Actual_payoff_20200505.parquet')\\\n",
    "                .select('servicecalendardate', 'ln_no', 'purchasepayoff_3_6')\n",
    "\n",
    "actual.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Join actual data with preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "target = 'purchasepayoff_3_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_all = df_preprocessed.join(actual, on=['servicecalendardate', 'ln_no'], how='inner').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------------------+\n",
      "|servicecalendardate|count(ln_no)|sum(purchasepayoff_3_6)|\n",
      "+-------------------+------------+-----------------------+\n",
      "|         2020-03-31|     1867297|                    0.0|\n",
      "|         2019-12-31|     1802243|                   44.0|\n",
      "|         2019-09-30|     1763391|                11183.0|\n",
      "|         2019-06-30|     1822001|                13684.0|\n",
      "|         2018-12-31|     1726017|                21914.0|\n",
      "|         2018-09-30|     1683583|                14008.0|\n",
      "|         2018-06-30|     1619695|                12303.0|\n",
      "|         2018-03-31|     1577779|                17018.0|\n",
      "|         2017-12-31|     1530004|                19389.0|\n",
      "|         2017-09-30|     1468441|                12459.0|\n",
      "|         2017-06-30|     1423608|                11440.0|\n",
      "|         2017-03-31|     1379094|                14992.0|\n",
      "|         2016-12-31|     1326375|                16483.0|\n",
      "+-------------------+------------+-----------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_all.groupby('servicecalendardate').agg(count('ln_no'), sum(target)).orderBy(col('servicecalendardate').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Under-sample negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# create fundtion for undersampling\n",
    "def data_split(df, train_months, target, rate):\n",
    "    \n",
    "    ## Split Train/Validation From Test ## \n",
    "    train_validate = df.where(col('servicecalendardate').isin(train_months))\n",
    "    \n",
    "    ## Split Out Payoffs into Train/Validate ##\n",
    "    train_refi_all = train_validate.filter(train_validate[target] == '1.')\n",
    "    train_refi, validate_refi = train_refi_all.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "    ## Split Out Non-Payoffs to Train/Validate\n",
    "    train_nonrefi_all = train_validate.filter(train_validate[target] == '0.')\n",
    "    train_nonrefi, validate_nonrefi = train_nonrefi_all.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "    ## Undersample Non-Payoffs ## \n",
    "    train_non, notused1 = train_nonrefi.randomSplit([rate, 1-rate], seed=123)\n",
    "    val_non, notused2 = validate_nonrefi.randomSplit([rate, 1-rate], seed=123)\n",
    "\n",
    "    ## Create Final Training/Validate Sets\n",
    "    train = train_refi.unionAll(train_non)\n",
    "    validate = validate_refi.unionAll(val_non)\n",
    "    train.cache()\n",
    "    \n",
    "    return train, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "train_months = ['2017-09-30', '2017-12-31', '2018-03-31', '2018-06-30', '2018-09-30', '2018-12-31']\n",
    "\n",
    "df_train, df_validate = data_split(df_all, train_months, target, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+\n",
      "|servicecalendardate|purchasepayoff_3_6|count|\n",
      "+-------------------+------------------+-----+\n",
      "|         2017-09-30|               0.0|25381|\n",
      "|         2017-09-30|               1.0| 8723|\n",
      "|         2017-12-31|               0.0|26400|\n",
      "|         2017-12-31|               1.0|13637|\n",
      "|         2018-03-31|               0.0|27208|\n",
      "|         2018-03-31|               1.0|11930|\n",
      "|         2018-06-30|               0.0|28176|\n",
      "|         2018-06-30|               1.0| 8680|\n",
      "|         2018-09-30|               0.0|29555|\n",
      "|         2018-09-30|               1.0| 9811|\n",
      "|         2018-12-31|               0.0|29972|\n",
      "|         2018-12-31|               1.0|15293|\n",
      "+-------------------+------------------+-----+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_train.groupby(['servicecalendardate', target]).count().orderBy('servicecalendardate', target).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def create_pipeline(target, *arg):\n",
    "    \n",
    "    exclude_cols = ('servicecalendardate', 'ln_no', 'og_note_dt', 'mosaic_group_pur_exp', 'ratespread_min_pur_exp',\n",
    "                'ln_purpose_type', 'investornameshort', 'ln_ann_int_rt', target) \n",
    "    cat_cols = [i[0] for i in df_train.dtypes if ((i[1]=='string') & (~i[0].endswith(exclude_cols)))]\n",
    "    num_cols = [i[0] for i in df_train.dtypes if ((i[1].startswith(('int', 'double'))) & (~i[0].endswith(exclude_cols)))]\n",
    "    \n",
    "    stages = []\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        \n",
    "        #Category indexing with StringIndexer\n",
    "        indexer = StringIndexer(inputCol = col, outputCol = col+'_idx').setHandleInvalid('keep')\n",
    "        stages += [indexer]\n",
    "        \n",
    "    #assemblerInputs = [c+'_vec' for c in cat_cols] + num_cols\n",
    "    assemblerInputs = [c+'_idx' for c in cat_cols] + num_cols\n",
    "    assembler = VectorAssembler(inputCols = assemblerInputs, outputCol = 'vectFeatures')\n",
    "    \n",
    "    stages += [assembler]\n",
    "    \n",
    "    lr = LogisticRegression(maxIter=100, regParam=0.1, elasticNetParam=0.0, fitIntercept = True,\n",
    "                            featuresCol='vectFeatures', labelCol=target)\n",
    "    rf = RandomForestClassifier(numTrees=250, maxDepth = 5, featuresCol='vectFeatures', labelCol=target)\n",
    "    gbt = GBTClassifier(maxIter=100, featuresCol='vectFeatures', labelCol=target)\n",
    "\n",
    "    pipeline_lr = Pipeline(stages = stages + [lr])\n",
    "    pipeline_rf = Pipeline(stages = stages + [rf])\n",
    "    pipeline_gbt = Pipeline(stages = stages + [gbt])\n",
    " \n",
    "    if \"lr\" in arg and \"rf\" in arg and \"gbt\" in arg:\n",
    "        return lr, rf, gbt, pipeline_lr, pipeline_rf, pipeline_gbt\n",
    "    elif \"lr\" in arg and \"rf\" in arg:\n",
    "        return lr, rf, pipeline_lr, pipeline_rf\n",
    "    elif \"lr\" in arg and \"gbt\" in arg:\n",
    "        return lr, gbt, pipeline_lr, pipeline_gbt \n",
    "    elif \"rf\" in arg and \"gbt\" in arg:\n",
    "        return rf, gbt, pipeline_rf, pipeline_gbt\n",
    "    elif \"rf\" in arg:\n",
    "        return rf, pipeline_rf\n",
    "    elif \"lr\" in arg:\n",
    "        return lr, pipeline_lr\n",
    "    elif \"gbt\" in arg:\n",
    "        return gbt, pipeline_gbt\n",
    "    else:\n",
    "        return gbt, pipeline_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fit_pipeline(pipeline, training_dataset):\n",
    "    model_pipeline = pipeline.fit(training_dataset)\n",
    "    return model_pipeline\n",
    "    \n",
    "def persist_modelpersist_(pipeline_model, model_name, model_save_path=model_save_path):\n",
    "    pipeline_model.save(model_save_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "lr, rf, gbt, pipeline_lr, pipeline_rf, pipeline_gbt = create_pipeline(target, \"lr\", \"rf\", \"gbt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Time: 200.39102458953857"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Logistic Regression\n",
    "start = time.time()\n",
    "model_pipeline_lr = fit_pipeline(pipeline_lr, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Logistic Regression Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Time: 209.19338965415955"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Random Forest\n",
    "start = time.time()\n",
    "model_pipeline_rf = fit_pipeline(pipeline_rf, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Random Forest Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Training Time: 3026.6029171943665"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Gradient Boosting\n",
    "start = time.time()\n",
    "model_pipeline_gbt = fit_pipeline(pipeline_gbt, df_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Gradient Boosting Training Time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "persist_modelpersist_(model_pipeline_lr, 'Mover_3to6_V1_1_LR_20200515')\n",
    "persist_modelpersist_(model_pipeline_rf, 'Mover_3to6_V1_1_RF_20200515')\n",
    "persist_modelpersist_(model_pipeline_gbt, 'Mover_3to6_V1_1_GBT_20200515')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "model_pipeline_lr = PipelineModel.load(path = model_save_path+'Mover_3to6_V1_1_LR_20200515')\n",
    "model_pipeline_rf = PipelineModel.load(path = model_save_path+'Mover_3to6_V1_1_RF_20200515')\n",
    "model_pipeline_gbt = PipelineModel.load(path = model_save_path+'Mover_3to6_V1_1_GBT_20200515')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "def extractFeatureImp(model_pipeline, df_train, featuresCol):\n",
    "    \n",
    "    featureImp = model_pipeline.stages[-1].featureImportances\n",
    "    transformed = model_pipeline.transform(df_train)\n",
    "    list_extract = []\n",
    "    \n",
    "    for i in transformed.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + transformed.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "        \n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    \n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "    return varlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx                         name  \\\n",
      "5    13                 ageinmon_exp   \n",
      "7    15              currentcltv_exp   \n",
      "10   18           LiveYears_long_exp   \n",
      "4    12           ratespread_min_exp   \n",
      "8    16                orig_fico_exp   \n",
      "6    14                og_mtg_am_exp   \n",
      "1     9               home_value_exp   \n",
      "17    3    mosaic_group_refi_exp_idx   \n",
      "13   21                    ln_tr_exp   \n",
      "20    6      ln_purpose_type_exp_idx   \n",
      "0     8                   ct_age_exp   \n",
      "9    17          LiveYears_short_exp   \n",
      "3    11       personnum_per_room_exp   \n",
      "21    7  og_occupy_stat_type_exp_idx   \n",
      "19    5     loanamortizationtype_idx   \n",
      "2    10                 p_edu_hs_exp   \n",
      "18    4  loantypedescription_exp_idx   \n",
      "16    2                 ct_3_exp_idx   \n",
      "12   20         issingleborrower_exp   \n",
      "15    1                 ct_2_exp_idx   \n",
      "14    0                 ct_1_exp_idx   \n",
      "11   19            LiveYears_grp_exp   \n",
      "\n",
      "                                                 vals     score  \n",
      "5                                                 NaN  0.115164  \n",
      "7                                                 NaN  0.112409  \n",
      "10                                                NaN  0.095413  \n",
      "4                                                 NaN  0.088727  \n",
      "8                                                 NaN  0.084655  \n",
      "6                                                 NaN  0.076080  \n",
      "1                                                 NaN  0.057258  \n",
      "17    [other, DEFJK, ACG, BHQ, IL, MNOPRS, __unknown]  0.052676  \n",
      "13                                                NaN  0.044073  \n",
      "20     [refinance, cashout refi, purchase, __unknown]  0.037184  \n",
      "0                                                 NaN  0.034998  \n",
      "9                                                 NaN  0.033660  \n",
      "3                                                 NaN  0.029426  \n",
      "21  [primary residence, investment property, secon...  0.027349  \n",
      "19                            [fixed, arm, __unknown]  0.023594  \n",
      "2                                                 NaN  0.023118  \n",
      "18                         [conv, fha, va, __unknown]  0.020343  \n",
      "16                            [0, 1, 2, 3, __unknown]  0.014943  \n",
      "12                                                NaN  0.012370  \n",
      "15                               [0, 1, 2, __unknown]  0.008180  \n",
      "14                                  [0, 1, __unknown]  0.004414  \n",
      "11                                                NaN  0.003966"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "gbt_feature_importance = extractFeatureImp(model_pipeline_gbt, df_train, \"vectFeatures\")\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(gbt_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def predict(model_pipeline, data):\n",
    "    \n",
    "    _scoreUdf = udf(lambda v: float(v[1]), DoubleType())\n",
    "    \n",
    "    prediction = model_pipeline.transform(data)\n",
    "    pred_df = prediction.withColumn('pred', _scoreUdf(prediction['probability']))\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def roc_auc(df, target):   \n",
    "    \n",
    "    pred_df_lr = predict(model_pipeline_lr, df)\n",
    "    pred_df_rf = predict(model_pipeline_rf, df)\n",
    "    pred_df_gbt = predict(model_pipeline_gbt, df)\n",
    "\n",
    "    ### Calculate ROC ###  \n",
    "    evalPred_lr = pred_df_lr.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "    evalPred_rf = pred_df_rf.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "    evalPred_gbt = pred_df_gbt.select(target, 'rawPrediction', 'prediction', 'probability')\\\n",
    "                                .withColumnRenamed(target, 'label')\n",
    "\n",
    "    evaluatorLR = BinaryClassificationEvaluator()\n",
    "    evaluatorRF = BinaryClassificationEvaluator()\n",
    "    evaluatorGBT = BinaryClassificationEvaluator()\n",
    "\n",
    "    print(\"Test Area Under ROC - LR: \" + str(evaluatorLR.evaluate(evalPred_lr, {evaluatorLR.metricName: \"areaUnderROC\"})))        \n",
    "    print(\"Test Area Under ROC - RF: \" + str(evaluatorRF.evaluate(evalPred_rf, {evaluatorRF.metricName: \"areaUnderROC\"})))        \n",
    "    print(\"Test Area Under ROC - GBT: \" + str(evaluatorGBT.evaluate(evalPred_gbt, {evaluatorGBT.metricName: \"areaUnderROC\"})))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC - LR: 0.6108436508396088\n",
      "Test Area Under ROC - RF: 0.6352566049206047\n",
      "Test Area Under ROC - GBT: 0.6785597367147729\n",
      "\n",
      "\n",
      "Test Area Under ROC - LR: 0.6065183020070458\n",
      "Test Area Under ROC - RF: 0.6287321804683813\n",
      "Test Area Under ROC - GBT: 0.6548889379759892\n",
      "\n",
      "\n",
      "Test Area Under ROC - LR: 0.5958793753206999\n",
      "Test Area Under ROC - RF: 0.6220089997108489\n",
      "Test Area Under ROC - GBT: 0.6399175834062103"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "test_months = ['2017-06-30', '2019-06-30', '2019-09-30']\n",
    "df_test = df_all.where(col('servicecalendardate').isin(test_months))\n",
    "\n",
    "for df in [df_train, df_validate, df_test]:\n",
    "    roc_auc(df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Capture Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def deciles(df, model, target):\n",
    "    \n",
    "    pred_df = predict(model, df)\n",
    "\n",
    "    pred_df = QuantileDiscretizer(numBuckets=10, inputCol=\"pred\", outputCol=\"decile\", relativeError=0.00001,\n",
    "                             handleInvalid=\"error\").fit(pred_df).transform(pred_df)\n",
    "    pred_df = pred_df.withColumn('decile', (10 - F.col('decile')).cast('int'))\n",
    "    \n",
    "    window_cumsum = Window.orderBy('decile').rangeBetween(Window.unboundedPreceding, 0)\n",
    "    total_target = pred_df.select(F.sum(target)).collect()[0][0]\n",
    "    df_out = pred_df\\\n",
    "        .groupBy('decile', )\\\n",
    "        .agg(F.count('ln_no').alias('decile_cnt'), F.sum(target).alias('payoff_cnt'))\\\n",
    "        .withColumn('cum_sum', F.sum('payoff_cnt').over(window_cumsum) / total_target)\\\n",
    "        .sort('decile')\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-30"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "date_label = test_months[0]\n",
    "print(date_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-30\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    142344|    2074.0|0.18129370629370628|\n",
      "|     2|    142374|    1603.0| 0.3214160839160839|\n",
      "|     3|    142366|    1429.0| 0.4463286713286713|\n",
      "|     4|    142370|    1206.0| 0.5517482517482517|\n",
      "|     5|    142326|    1107.0|  0.648513986013986|\n",
      "|     6|    142378|    1058.0| 0.7409965034965035|\n",
      "|     7|    142362|     943.0| 0.8234265734265734|\n",
      "|     8|    142347|     818.0|   0.89493006993007|\n",
      "|     9|    142377|     679.0| 0.9542832167832168|\n",
      "|    10|    142364|     523.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(date_label)\n",
    "deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_lr, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-30\n",
      "+------+----------+----------+-------------------+\n",
      "|decile|decile_cnt|payoff_cnt|            cum_sum|\n",
      "+------+----------+----------+-------------------+\n",
      "|     1|    142357|    2256.0| 0.1972027972027972|\n",
      "|     2|    142372|    1739.0| 0.3492132867132867|\n",
      "|     3|    142359|    1464.0|0.47718531468531467|\n",
      "|     4|    142333|    1287.0| 0.5896853146853147|\n",
      "|     5|    142362|    1125.0| 0.6880244755244755|\n",
      "|     6|    142470|     979.0| 0.7736013986013986|\n",
      "|     7|    142270|     928.0| 0.8547202797202798|\n",
      "|     8|    142355|     727.0| 0.9182692307692307|\n",
      "|     9|    142357|     597.0| 0.9704545454545455|\n",
      "|    10|    142373|     338.0|                1.0|\n",
      "+------+----------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(date_label)\n",
    "deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_rf, target).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "print(date_label)\n",
    "deciles(df_all.filter(col('servicecalendardate') == date_label), model_pipeline_gbt, target).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fullMonthPrediction(df):\n",
    "    \n",
    "    pred_df_lr = predict(model_pipeline_lr, df)\n",
    "    validationPredictionsLR = pred_df_lr.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'logRegProb')\n",
    "    \n",
    "    pred_df_rf = predict(model_pipeline_rf, df)\n",
    "    validationPredictionsRF = pred_df_rf.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'randForProb')\n",
    "    \n",
    "    pred_df_gbt = predict(model_pipeline_gbt, df)\n",
    "    validationPredictionsGBT = pred_df_gbt.select('ln_no', 'pred')\\\n",
    "                                            .withColumnRenamed('pred', 'gbtProb')\n",
    "    \n",
    "    combinedFinalPred = validationPredictionsLR.join(validationPredictionsRF, on='ln_no', how='left')\\\n",
    "                                                .join(validationPredictionsGBT, on='ln_no', how='left')\\\n",
    "                                                .dropDuplicates()\n",
    "    \n",
    "    return combinedFinalPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def pred_save(df, servicedate, filename, result_path = result_path):\n",
    "    \n",
    "    df1 = df.where(col('servicecalendardate') == servicedate)\n",
    "    df_pred = fullMonthPrediction(df1)\n",
    "    \n",
    "    df_pred.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def pred_save_newmonth(preprocessed_file, filename, monthly_prep_path=monthly_prep_path, result_path=result_path):\n",
    "    \n",
    "    df = spark.read.parquet(monthly_prep_path + preprocessed_file)\n",
    "    df_pred = fullMonthPrediction(df)\n",
    "    \n",
    "    print(df.count())\n",
    "    df_pred.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 2017-6\n",
    "pred_save(df_all, '2017-06-30', 'pred_MoverV1_1_jun17_20200514.csv')\n",
    "\n",
    "# 2017-9\n",
    "pred_save(df_all, '2017-09-30', 'pred_MoverV1_1_sep17_20200514.csv')\n",
    "\n",
    "# 2017-12\n",
    "pred_save(df_all, '2017-12-31', 'pred_MoverV1_1_dec17_20200514.csv')\n",
    "\n",
    "# 2018-3\n",
    "pred_save(df_all, '2018-03-31', 'pred_MoverV1_1_mar18_20200514.csv')\n",
    "\n",
    "# 2018-6\n",
    "pred_save(df_all, '2018-06-30', 'pred_MoverV1_1_jun18_20200514.csv')\n",
    "\n",
    "# 2018-9\n",
    "pred_save(df_all, '2018-09-30', 'pred_MoverV1_1_sep18_20200514.csv')\n",
    "\n",
    "# 2018-12\n",
    "pred_save(df_all, '2018-12-31', 'pred_MoverV1_1_dec18_20200514.csv')\n",
    "\n",
    "# 2019-03\n",
    "pred_save(df_all, '2019-03-31', 'pred_MoverV1_1_mar19_20200514.csv')\n",
    "\n",
    "# 2019-06\n",
    "pred_save(df_all, '2019-06-30', 'pred_MoverV1_1_jun19_20200514.csv')\n",
    "\n",
    "# 2019-09\n",
    "pred_save(df_all, '2019-09-30', 'pred_MoverV1_1_sep19_20200514.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 2018-1\n",
    "pred_save_newmonth('monthly_preprocessed_201801.parquet', 'pred_MoverV3_2_jan18_20190722.csv')\n",
    "\n",
    "# 2018-2\n",
    "pred_save_newmonth('monthly_preprocessed_201802.parquet', 'pred_MoverV3_2_feb18_20190722.csv')\n",
    "\n",
    "# 2018-4\n",
    "pred_save_newmonth('monthly_preprocessed_201804.parquet', 'pred_MoverV3_2_apr18_20190722.csv')\n",
    "\n",
    "# 2018-5\n",
    "pred_save_newmonth('monthly_preprocessed_201805.parquet', 'pred_MoverV3_2_may18_20190722.csv')\n",
    "\n",
    "# 2018-7\n",
    "pred_save_newmonth('monthly_preprocessed_201807.parquet', 'pred_MoverV3_2_jul18_20190722.csv')\n",
    "\n",
    "# 2018-8\n",
    "pred_save_newmonth('monthly_preprocessed_201808.parquet', 'pred_MoverV3_2_aug18_20190722.csv')\n",
    "\n",
    "# 2018-10\n",
    "pred_save_newmonth('monthly_preprocessed_201810.parquet', 'pred_MoverV3_2_oct18_20190722.csv')\n",
    "\n",
    "# 2018-11\n",
    "pred_save_newmonth('monthly_preprocessed_201811.parquet', 'pred_MoverV3_2_nov18_20190722.csv')\n",
    "\n",
    "# 2019-1\n",
    "pred_save_newmonth('monthly_preprocessed_201901.parquet', 'pred_MoverV3_2_jan19_20190722.csv')\n",
    "\n",
    "# 2019-2\n",
    "pred_save_newmonth('monthly_preprocessed_201902.parquet', 'pred_MoverV3_2_feb19_20190722.csv')\n",
    "\n",
    "# 2019-4\n",
    "pred_save_newmonth('monthly_preprocessed_201904.parquet', 'pred_MoverV3_2_apr19_20190722.csv')\n",
    "\n",
    "# 2019-5\n",
    "pred_save_newmonth('monthly_preprocessed_201905.parquet', 'pred_MoverV3_2_may19_20190722.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
