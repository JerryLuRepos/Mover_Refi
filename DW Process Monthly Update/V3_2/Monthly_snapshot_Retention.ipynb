{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success configuring sparkmagic livy.\n",
      "['https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1']\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "from dsx_core_utils import proxy_util,dsxhi_util\n",
    "proxy_util.configure_proxy_livy()\n",
    "dsxhi_util.list_livy_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\"executorCores\": 4, \"numExecutors\": 5, \"executorMemory\": \"10g\", \n",
    " \"driverMemory\": \"8g\", \"proxyUser\": \"jchen-\", \"driverCores\": 1, \n",
    " \"conf\": {\"spark.yarn.appMasterEnv.THEANO_FLAGS\": \"base_compiledir=${PWD}/.theano\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>15164</td><td>application_1571281213559_0756</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://qlawsbidlhm01b.ad.datalake.foc.zone:8090/proxy/application_1571281213559_0756/\">Link</a></td><td><a target=\"_blank\" href=\"https://qlawsbidlhw14a.ad.datalake.foc.zone:8042/node/containerlogs/container_e479_1571281213559_0756_01_000001/jchen-\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s servicingdata -k -l python -u https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0.2.6.5.0-292"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "import pyspark\n",
    "import os, sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when, lit, countDistinct, max, min\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "import pyspark.sql.types as T\n",
    "from functools import reduce\n",
    "\n",
    "from os.path import expanduser, join, abspath\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "csv_path = '/dev/projects/retention_models/csv_data/'\n",
    "#hpi_data_save_path = '/dev/projects/refi_lead_generation/datasets_old/'\n",
    "monthly_path = '/dev/projects/retention_models/monthly_snapshot/'\n",
    "monthly_prep_path = '/dev/projects/retention_models/monthly_snapshot/monthly_preprocessed/'\n",
    "actual_path = '/dev/projects/retention_models/actual_value/'\n",
    "hpi_path = '/dev/projects/retention_models/hpi/'\n",
    "\n",
    "#Refi\n",
    "model_save_path_refi = '/dev/projects/retention_models/refi_payoff/models/'\n",
    "result_path_refi = '/dev/projects/retention_models/refi_payoff/training/results/'\n",
    "\n",
    "#Pur\n",
    "model_save_path_pur = '/dev/projects/retention_models/purchase_payoff/models/'\n",
    "result_path_pur = '/dev/projects/retention_models/purchase_payoff/training/results/movermodel/'\n",
    "result_path_pur_6 = '/dev/projects/retention_models/purchase_payoff/training/results/mover_6/'\n",
    "result_path_pur_12 = '/dev/projects/retention_models/purchase_payoff/training/results/mover_12/'\n",
    "result_path_pur_all = '/dev/projects/retention_models/purchase_payoff/training/results/'\n",
    "\n",
    "#prod\n",
    "prod_pur_path = '/prod/projects/model_conformed/RocketScience/LeadGen/PurLeadGen/1.0/'\n",
    "prod_refi_path = '/prod/projects/model_conformed/RocketScience/LeadGen/RefiLeadGen/1.0/'\n",
    "file_path = '/prod/projects/model_conformed/RocketScience/LeadGen/Currentfile/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Before running new month) check left-portfolio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763391\n",
      "+-------------------+----------+\n",
      "|servicecalendardate|     ln_no|\n",
      "+-------------------+----------+\n",
      "|         2019-09-30|3431477187|\n",
      "+-------------------+----------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_list = spark.read.csv(os.path.join(csv_path, 'servicing_loanlist_201909.csv'), header=True)\n",
    "\n",
    "print(df_list.count())\n",
    "df_list.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1773361"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_book = spark.sql('''\n",
    "select distinct ln_no\n",
    "from data_science_sandbox.servicing_dbo_payoff_source_purchase_client_crdtrs \n",
    "where servicecalendardate = '{0}'\n",
    "and leftportfoliodate is null\n",
    "'''.format('2019-09-30'))\n",
    "\n",
    "df_book.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763391"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df1 = df_book.join(df_list, on='ln_no', how='inner')\n",
    "\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read Servicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def retrieve_dataset(servicedate, loanlist_file):\n",
    "    \n",
    "    spark.sql('use data_science_sandbox') \n",
    "    df_raw = spark.sql('select * from servicing_dbo_payoff_source_purchase_client_crdtrs where leftportfoliodate is null')\\\n",
    "                    .where(col('servicecalendardate') == servicedate)\n",
    "    \n",
    "    #servicing book loan list, to remove the loans already left portfolio\n",
    "    df_list = spark.read.csv(os.path.join(csv_path, loanlist_file), header=True)\n",
    "    \n",
    "    df_ser = df_raw.join(df_list, on=['servicecalendardate', 'ln_no'], how='inner').drop_duplicates()\n",
    "    \n",
    "    return df_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def retrieve_dataset_dw(servicedate):\n",
    "    \n",
    "    spark.sql('use data_science_sandbox') \n",
    "    df_raw = spark.sql('select * from servicing_dbo_payoff_source_purchase_client_crdtrs')\\\n",
    "                    .where(col('servicecalendardate') == servicedate)\\\n",
    "                    .drop_duplicates()\n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_ser = retrieve_dataset(servicedate='2019-09-30', loanlist_file='servicing_loanlist_201909.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_ser = retrieve_dataset_dw(servicedate='2016-06-30')\n",
    "#df_ser.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+------------+\n",
      "|servicecalendardate|count(DISTINCT ln_no)|count(ln_no)|\n",
      "+-------------------+---------------------+------------+\n",
      "|         2019-09-30|              1763391|     1763391|\n",
      "+-------------------+---------------------+------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_ser.groupby('servicecalendardate').agg(F.countDistinct('ln_no'), F.count('ln_no')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026611"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_ser.where((col('mosaichousehold').isNotNull()) & (col('mosaichousehold') != '')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|p_age_31_50|  count|\n",
      "+-----------+-------+\n",
      "|          1| 333026|\n",
      "|          6|     98|\n",
      "|          3|  47974|\n",
      "|          5|    937|\n",
      "|          4|   8070|\n",
      "|          2| 206028|\n",
      "|          0|1167258|\n",
      "+-----------+-------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_ser.groupby('p_age_31_50').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_new = df_ser.select('servicecalendardate', 'og_note_dt', 'ln_no',\n",
    "                        'loantypedescription', 'unpaidbalfirstsecond', 'loanamortizationtype', 'og_mtg_am',\n",
    "                        'ln_purpose_type', 'og_occupy_stat_type',\n",
    "                        'ln_ann_int_rt', 'ln_tr', 'ln_monthly_pmt_am', 'ln_eloc_cd',\n",
    "                        'orig_ltv_ratio_fc', 'pr_value_am', 'investornameshort',\n",
    "                        'issingleborrower', 'liveyears', 'pr_zip_cd',\n",
    "                        'lengthresidence', 'personnum_unit', 'estimatedhomevalue', 'mosaichousehold', 'homebedroomcnt',\n",
    "                        'p1_combinedage', 'p_age_31_50', 'p_age_50more', 'p_edu_hs', 'p_married', 'quarteryearkey',\n",
    "                        'mostrecentcredittriggerage', 'triggercnt1monthprior', 'triggercnt2monthprior', 'triggercnt3monthprior'\n",
    "                       )\n",
    "\n",
    "#'gcid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (2) Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def retrieve_marketdatafact():\n",
    "    \"\"\"Retrieve market interest rates from DL\"\"\"\n",
    "    \n",
    "    df = spark.sql('select * from lock2close_raw_access.marketdatafact_l2c_orc')\\\n",
    "                .where(F.to_date(col('dateid'), 'yyyy-MM-dd') >= '2015-12-01')\\\n",
    "                .withColumn('yearmonth', col('dateid').substr(1, 7))\n",
    "    \n",
    "    df_market = df.groupby('yearmonth')\\\n",
    "                    .agg(F.min('currentfannie30yryield').alias('fannie30_min'),\n",
    "                         F.min('currentfannie15yryield').alias('fannie15_min'),\n",
    "                         F.min('currentginny30yryield').alias('ginny30_min'))\n",
    "    \n",
    "    return F.broadcast(df_market)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_market_agg = retrieve_marketdatafact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+-----------+\n",
      "|yearmonth|fannie30_min|fannie15_min|ginny30_min|\n",
      "+---------+------------+------------+-----------+\n",
      "|  2019-07|       2.595|       2.215|      2.348|\n",
      "|  2019-08|       2.389|       2.033|      2.184|\n",
      "|  2019-09|       2.298|       1.925|      2.102|\n",
      "+---------+------------+------------+-----------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_market_agg.where(F.col('yearmonth').isin('2019-08', '2019-09', '2019-07')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (*) Update HPI (Once necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+-----+--------------------+\n",
      "|zip|year|quarter|index|                type|\n",
      "+---+----+-------+-----+--------------------+\n",
      "| 10|1995|      1|  100|Native 3-Digit ZI...|\n",
      "+---+----+-------+-----+--------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "hpi_raw = spark.read.csv(csv_path + 'hpi_2019q2.csv', header=True)\n",
    "hpi_raw.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+-----+--------------------+-----+\n",
      "|zip|year|quarter|index|                type|month|\n",
      "+---+----+-------+-----+--------------------+-----+\n",
      "| 10|1995|      1|  100|Native 3-Digit ZI...|    1|\n",
      "+---+----+-------+-----+--------------------+-----+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "expr_mon1 = when(col('quarter')=='1', '1')\\\n",
    "                .otherwise(\n",
    "                    when(col('quarter')=='2', '4')\\\n",
    "                        .otherwise(\n",
    "                            when(col('quarter')=='3', '7')\\\n",
    "                                .otherwise('10')))\n",
    "    \n",
    "expr_mon2 = when(col('quarter')=='1', '2')\\\n",
    "                .otherwise(\n",
    "                    when(col('quarter')=='2', '5')\\\n",
    "                        .otherwise(\n",
    "                            when(col('quarter')=='3', '8')\\\n",
    "                                .otherwise('11')))\n",
    "\n",
    "expr_mon3 = when(col('quarter')=='1', '3')\\\n",
    "                .otherwise(\n",
    "                    when(col('quarter')=='2', '6')\\\n",
    "                        .otherwise(\n",
    "                            when(col('quarter')=='3', '9')\\\n",
    "                                .otherwise('12')))\n",
    "\n",
    "raw1 = hpi_raw.withColumn('month', expr_mon1)\n",
    "raw2 = hpi_raw.withColumn('month', expr_mon2)\n",
    "raw3 = hpi_raw.withColumn('month', expr_mon3)\n",
    "\n",
    "hpi_new = raw1.union(raw2).union(raw3)\n",
    "\n",
    "hpi_new.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "hpi_new.write.parquet(hpi_path + 'hpi.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+-----+--------------------+-----+\n",
      "|zip|year|quarter|index|                type|month|\n",
      "+---+----+-------+-----+--------------------+-----+\n",
      "| 10|1995|      1|  100|Native 3-Digit ZI...|    2|\n",
      "+---+----+-------+-----+--------------------+-----+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "hpi = spark.read.parquet(hpi_path + 'hpi.parquet')\n",
    "\n",
    "hpi.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) HPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "  ## Loads the HPI based on 3-digit zip (for estimating home value)## Loads \n",
    "\n",
    "def preprocess_hpi(pullmonth, usemax=True):\n",
    "    \n",
    "    # If it's for current month, set usemax=True, which means the most recent as the current value.\n",
    "    # If it's for historical data, set usemax=False, which means the pull month is the current value.\n",
    "       \n",
    "    expr_padded_month = F.lpad(col('month'), 2, '0')\n",
    "    \n",
    "    df_hpi = spark.read.parquet(hpi_path + 'hpi.parquet')\\\n",
    "                    .withColumn('hpi_yearmonth', F.concat(col('year'), F.lit('-'), expr_padded_month))\\\n",
    "                    .withColumn('hpi_zip', F.lpad(col('zip'), 3, '0'))\n",
    "    \n",
    "    df_hpi_max_date = df_hpi.groupby('hpi_zip').agg(F.max('hpi_yearmonth').alias('yearmonth_max'))\\\n",
    "                            .withColumn('yearmonth_pull', F.lit(pullmonth))\n",
    "    \n",
    "    if not usemax:\n",
    "        df_hpi_curr_date = df_hpi_max_date.withColumnRenamed('yearmonth_pull', 'hpi_yearmonth')\n",
    "    else:\n",
    "        df_hpi_curr_date = df_hpi_max_date.withColumnRenamed('yearmonth_max', 'hpi_yearmonth')\n",
    "        \n",
    "    df_hpi_current = df_hpi.join(df_hpi_curr_date, on=['hpi_yearmonth', 'hpi_zip'], how='inner')\\\n",
    "                        .withColumnRenamed('index', 'hpi_current')\\\n",
    "                        .select('hpi_zip', 'hpi_current')\n",
    "    \n",
    "    df_hpi = df_hpi.join(df_hpi_current, on='hpi_zip', how='left')\\\n",
    "                .select('hpi_zip', 'hpi_yearmonth', 'index', 'hpi_current')\\\n",
    "                .withColumnRenamed('index', 'hpi_og')\n",
    "    \n",
    "    df_hpi_avg = df_hpi.groupby('hpi_yearmonth')\\\n",
    "                        .agg(F.avg('hpi_og').cast('float').alias('hpi_avg'), \n",
    "                             F.avg('hpi_current').cast('float').alias('hpi_current_avg'))\\\n",
    "                        .withColumnRenamed('hpi_yearmonth', 'hpi_yearmonth_avg')\n",
    "\n",
    "    return F.broadcast(df_hpi), F.broadcast(df_hpi_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_hpi, df_hpi_avg = preprocess_hpi(pullmonth='2019-09', usemax=True)\n",
    "\n",
    "#historical\n",
    "#df_hpi, df_hpi_avg = preprocess_hpi(pullmonth='2016-06', usemax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------+-----------+\n",
      "|hpi_zip|hpi_yearmonth|hpi_og|hpi_current|\n",
      "+-------+-------------+------+-----------+\n",
      "|    296|      2019-06|221.02|     221.02|\n",
      "|    467|      2019-06|194.75|     194.75|\n",
      "|    675|      2019-06|194.05|     194.05|\n",
      "+-------+-------------+------+-----------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_hpi.where(F.col('hpi_yearmonth')=='2019-06').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+---------------+\n",
      "|hpi_yearmonth_avg|  hpi_avg|hpi_current_avg|\n",
      "+-----------------+---------+---------------+\n",
      "|          1999-10|121.28368|      230.30124|\n",
      "|          2013-05|178.57294|      230.30124|\n",
      "|          2009-07|183.08472|      230.30124|\n",
      "+-----------------+---------+---------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_hpi_avg.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Update FICO table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_fico = spark.read.csv(csv_path + 'servicing_fico_20191004.csv', header=True)\\\n",
    "                .select('ln_no', col('orig_fico').cast('int'))\n",
    "    \n",
    "df_fico.write.mode('overwrite').saveAsTable('data_science_sandbox.servicing_fico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374545\n",
      "+----------+---------+\n",
      "|     ln_no|orig_fico|\n",
      "+----------+---------+\n",
      "|3330783959|      757|\n",
      "+----------+---------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "fico_check = spark.sql('select * from data_science_sandbox.servicing_fico')\n",
    "\n",
    "print(fico_check.count())\n",
    "fico_check.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Combine all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def combine_tables(df, mdf = df_market_agg, df_hpi = df_hpi, df_hpi_avg=df_hpi_avg):\n",
    "    \n",
    "    df = df.withColumn('yearmonth', col('servicecalendardate').substr(1, 7))\\\n",
    "            .withColumn('og_yearmonth', col('og_note_dt').substr(1, 7))\\\n",
    "            .withColumn('zip3', col('pr_zip_cd').substr(1, 3))\n",
    "    \n",
    "    #Join market data\n",
    "    df = df.join(mdf, on='yearmonth', how = 'left')\n",
    "    \n",
    "    #Add FICO to the table\n",
    "    df_fico = spark.sql('select * from data_science_sandbox.servicing_fico')\n",
    "    df = df.join(df_fico, on='ln_no', how='left')\n",
    "    \n",
    "    #Join the hpi data\n",
    "    df = df.join(df_hpi, (df.og_yearmonth == df_hpi.hpi_yearmonth) & (df.zip3 == df_hpi.hpi_zip), how='left')\\\n",
    "            .withColumnRenamed('index', 'hpi_og')\n",
    "            \n",
    "    #Use the average to manipulate NULL hpi           \n",
    "    df = df.join(df_hpi_avg, df.og_yearmonth == df_hpi_avg.hpi_yearmonth_avg, how='left')\n",
    "    \n",
    "    #HPI columns and HPI change\n",
    "    expr_hpi_og = F.when(F.col('hpi_og').isNull(), F.col('hpi_avg')).otherwise(F.col('hpi_og'))\n",
    "\n",
    "    expr_hpi_curr = F.when(F.col('hpi_current').isNull(), F.col('hpi_current_avg')).otherwise(F.col('hpi_current'))\n",
    "\n",
    "    expr_hpi_change = F.when(F.col('hpi_avg').isNull(), 1.0).otherwise(expr_hpi_curr/expr_hpi_og)\n",
    "\n",
    "    df = df.withColumn('hpi_og_exp', expr_hpi_og)\\\n",
    "            .withColumn('hpi_curr_exp', expr_hpi_curr)\\\n",
    "            .withColumn('hpi_change_exp', expr_hpi_change)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_all = combine_tables(df_new)\n",
    "\n",
    "#df_all.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#Save a copy for quick analysis\n",
    "df_all.write.parquet(os.path.join(monthly_path, 'servicing_df_all_sep19_20191004.parquet'), mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6). Preprocess Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_all = spark.read.parquet(os.path.join(monthly_path, 'servicing_df_all_sep19_20191004.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def preprocess_dataset(df):\n",
    "    \n",
    "    ##credit trigger\n",
    "    expr_ct_age = F.when(F.col('mostrecentcredittriggerage').isNull(), 0)\\\n",
    "                            .otherwise(\n",
    "                                F.when(F.col('mostrecentcredittriggerage').isin(['NULL', 'null', '0']), 0)\\\n",
    "                                    .otherwise(F.col('mostrecentcredittriggerage') + 1\n",
    "                                            )\n",
    "                                    )\n",
    "    expr_ct_1_mo = F.when(F.col('triggercnt1monthprior').isNull(), '0')\\\n",
    "                            .otherwise(\n",
    "                                F.when(F.col('triggercnt1monthprior').isin(['NULL', 'null', '0']), '0')\\\n",
    "                                    .otherwise(\n",
    "                                        F.when(F.col('triggercnt1monthprior') > 1, '1')\\\n",
    "                                            .otherwise(F.col('triggercnt1monthprior')).cast('string')\n",
    "                                            )\n",
    "                                    )\n",
    "                                  \n",
    "    expr_ct_2_mo = F.when(F.col('triggercnt2monthprior').isNull(), '0')\\\n",
    "                            .otherwise(\n",
    "                                F.when(F.col('triggercnt2monthprior').isin(['NULL', 'null', '0']), '0')\\\n",
    "                                    .otherwise(\n",
    "                                        F.when(F.col('triggercnt2monthprior') > 2, '2')\\\n",
    "                                            .otherwise(F.col('triggercnt2monthprior')).cast('string')\n",
    "                                            )\n",
    "                                    )\n",
    "        \n",
    "    expr_ct_3_mo = F.when(F.col('triggercnt3monthprior').isNull(), '0')\\\n",
    "                            .otherwise(\n",
    "                                F.when(F.col('triggercnt3monthprior').isin(['NULL', 'null', '0']), '0')\\\n",
    "                                    .otherwise(\n",
    "                                        F.when(F.col('triggercnt3monthprior') > 3, '3')\\\n",
    "                                            .otherwise(F.col('triggercnt3monthprior')).cast('string')\n",
    "                                            )\n",
    "                                    )\n",
    "    \n",
    "    ##Experian\n",
    "    expr_home_value = F.when(F.col('EstimatedHomeValue').isNull(), 271225.)\\\n",
    "                            .otherwise(\n",
    "                                F.when(F.col('EstimatedHomeValue').isin(['NULL', 'null', '0']), 271225.)\\\n",
    "                                    .otherwise(F.col('EstimatedHomeValue')\n",
    "                                            )\n",
    "                                    )\n",
    "        \n",
    "    expr_age_31_50 = F.when(F.col('p_age_31_50').isNull(), 0)\\\n",
    "                        .otherwise(\n",
    "                            F.when(F.col('p_age_31_50').isin(['null', 'NULL']), 0)\\\n",
    "                                .otherwise(F.col('p_age_31_50').cast('int')))\n",
    "    \n",
    "    expr_edu_hs = F.when(F.col('p_edu_hs').isNull(), 0)\\\n",
    "                        .otherwise(\n",
    "                            F.when(F.col('p_edu_hs').isin(['null', 'NULL']), 0)\\\n",
    "                                .otherwise(F.col('p_edu_hs').cast('int')))\n",
    "    \n",
    "    expr_married = F.when(F.col('p_married').isNull(), 0)\\\n",
    "                        .otherwise(\n",
    "                            F.when(F.col('p_married').isin(['null', 'NULL']), 0)\\\n",
    "                                .otherwise(F.col('p_married').cast('int')))\n",
    "        \n",
    "    expr_bedroomcnt = F.when(F.col('homebedroomcnt').isNull(), 3)\\\n",
    "                            .otherwise(\n",
    "                                F.when(F.col('homebedroomcnt').isin(['NULL', 'null', '0']), 3)\\\n",
    "                                    .otherwise(\n",
    "                                        F.when(F.col('homebedroomcnt').isin(['1', '2', '3', '4', '5']), F.col('homebedroomcnt').cast('int'))\\\n",
    "                                            .otherwise(6)\n",
    "                                            )\n",
    "                                    )\n",
    "                             \n",
    "    expr_person_per_room = F.when(F.col('personnum_unit').isNull(), 3 / expr_bedroomcnt)\\\n",
    "                                .otherwise(\n",
    "                                    F.when(F.col('personnum_unit').isin(['null', 'NULL', '0']), 3 / expr_bedroomcnt)\\\n",
    "                                        .otherwise(F.col('personnum_unit') / expr_bedroomcnt)\n",
    "                                )\n",
    "    \n",
    "    #Different mosaic groups from the one in mover model!!\n",
    "    expr_mosaicgroup = F.when(F.col('mosaichousehold').isin(['Null', 'null']), 'other')\\\n",
    "                            .otherwise(F.col('mosaichousehold').substr(0, 1))\n",
    "        \n",
    "    expr_mosaicgroup_refi_clust = F.when(expr_mosaicgroup.isin(['B', 'H', 'Q']), 'BHQ')\\\n",
    "                                   .otherwise(\n",
    "                                       F.when(expr_mosaicgroup.isin(['A', 'C', 'G']), 'ACG')\\\n",
    "                                           .otherwise(\n",
    "                                               F.when(expr_mosaicgroup.isin(['D', 'E', 'F', 'J', 'K']), 'DEFJK')\\\n",
    "                                                   .otherwise(\n",
    "                                                       F.when(expr_mosaicgroup.isin(['I', 'L']), 'IL')\\\n",
    "                                                           .otherwise(\n",
    "                                                               F.when(expr_mosaicgroup.isin(['M', 'N', 'O', 'P', 'R', 'S']), 'MNOPRS')\\\n",
    "                                                                   .otherwise('other')\n",
    "                                                           )\n",
    "                                                   )\n",
    "                                               )\n",
    "                                   )\n",
    "        \n",
    "    expr_mosaicgroup_pur_clust = F.when(expr_mosaicgroup.isin(['A', 'B']), 'AB')\\\n",
    "                                   .otherwise(\n",
    "                                       F.when(expr_mosaicgroup.isin(['C', 'D', 'E']), 'CDE')\\\n",
    "                                           .otherwise(\n",
    "                                               F.when(expr_mosaicgroup.isin(['F', 'G']), 'FG')\\\n",
    "                                                   .otherwise(\n",
    "                                                       F.when(expr_mosaicgroup.isin(['I', 'J']), 'IJ')\\\n",
    "                                                           .otherwise('other')\n",
    "                                                   )\n",
    "                                            )\n",
    "                                   )    \n",
    "        \n",
    "    ##Market Rate\n",
    "    expr_rate_spread_min = F.when(F.col('ln_ann_int_rt').isNull(), 0.0)\\\n",
    "                                .otherwise(\n",
    "                                    F.when(F.col('ln_ann_int_rt').isin(['null', 'NULL']), 0.0)\\\n",
    "                                        .otherwise(F.when(F.col('loantypedescription') == 'CONV', \n",
    "                                                         F.when(F.col('ln_tr') == '360', \n",
    "                                                                F.col('ln_ann_int_rt')*100 - F.col('fannie30_min'))\\\n",
    "                                                                .otherwise(F.col('ln_ann_int_rt')*100 - F.col('fannie15_min'))\n",
    "                                                         )\\\n",
    "                                                   .otherwise(\n",
    "                                                       F.when(F.col('loantypedescription').isin(['FHA', 'VA']), \n",
    "                                                              F.col('ln_ann_int_rt')*100 - F.col('ginny30_min'))\\\n",
    "                                                               .otherwise(F.col('ln_ann_int_rt')*100 - F.col('fannie30_min')))\n",
    "                                                   )\n",
    "                                            )\n",
    "\n",
    "    # This is only for Mover V3 and before \n",
    "    expr_rate_spread_min_pur = F.when(F.col('ln_ann_int_rt').isNull(), 0.0)\\\n",
    "                                .otherwise(\n",
    "                                    F.when(F.col('ln_ann_int_rt').isin(['null', 'NULL']), 0.0)\\\n",
    "                                        .otherwise(F.when(F.col('loantypedescription') == 'CONV', \n",
    "                                                         F.when(F.col('ln_tr') == '360', \n",
    "                                                                 F.col('fannie30_min') - F.col('ln_ann_int_rt')*100)\\\n",
    "                                                                .otherwise(F.col('fannie15_min') - F.col('ln_ann_int_rt')*100)\n",
    "                                                         )\\\n",
    "                                                   .otherwise(\n",
    "                                                       F.when(F.col('loantypedescription').isin(['FHA', 'VA']), \n",
    "                                                               F.col('ginny30_min') - F.col('ln_ann_int_rt')*100)\\\n",
    "                                                               .otherwise(F.col('fannie15_min')) - F.col('ln_ann_int_rt')*100)\n",
    "                                                   )\n",
    "                                            )\n",
    "\n",
    "    expr_borrowerage_group = F.when(F.col('p1_combinedage').isNull(), 'median')\\\n",
    "                                .otherwise(\n",
    "                                    F.when(F.col('p1_combinedage').isin(['Null', 'null']), 'median')\\\n",
    "                                        .otherwise(\n",
    "                                            F.when(F.col('p1_combinedage').substr(2, 2).cast('double') > 40.0, 'old')\\\n",
    "                                                .otherwise(\n",
    "                                                    F.when(F.col('p1_combinedage').substr(2, 2).cast('double') < 25.0, 'young')\\\n",
    "                                                        .otherwise('median')\n",
    "                                                )\n",
    "                                        )\n",
    "                                )\n",
    "    \n",
    "    ##Servicing Data       \n",
    "    expr_loan_type = F.when(F.col('loantypedescription').isin(['FHA', 'VA']), F.lower(F.col('loantypedescription')))\\\n",
    "                        .otherwise('conv')\n",
    "    \n",
    "    expr_ageinmon = F.when(F.col('og_note_dt').isNull(), 83)\\\n",
    "                        .otherwise(\n",
    "                            F.when(F.col('og_note_dt').isin(['null', 'NULL']), 83)\\\n",
    "                                .otherwise(F.months_between(F.col('servicecalendardate'), F.col('og_note_dt')))\n",
    "                        )        \n",
    "    \n",
    "    #Currentcltv (use HPI change)\n",
    "    expr_og_mtg_am = when(col('og_mtg_am').isNull(), 173200.)\\\n",
    "                        .otherwise(\n",
    "                            when(col('og_mtg_am').isin(['NULL', 'null']), 173200.)\n",
    "                                .otherwise(\n",
    "                                    col('og_mtg_am').cast('double')\n",
    "                                    )\n",
    "                            )\n",
    "\n",
    "    expr_og_homevalue = when((col('pr_value_am').isNotNull()) & (~col('pr_value_am').isin(['null', 'NULL', '0', 0])), col('pr_value_am'))\\\n",
    "                            .otherwise(\n",
    "                                when(col('orig_ltv_ratio_fc').isNull(), 0.)\\\n",
    "                                    .otherwise(\n",
    "                                        when(col('orig_ltv_ratio_fc').isin(['null', 'NULL', '0', 0]), 0.)\\\n",
    "                                            .otherwise(expr_og_mtg_am / col('orig_ltv_ratio_fc'))\n",
    "                                    )\n",
    "                            )\n",
    "\n",
    "    expr_currentcltv_cal = when(col('unpaidbalfirstsecond').isNull(), 0.)\\\n",
    "                                .otherwise(\n",
    "                                    when(col('unpaidbalfirstsecond').isin(['null', 'NULL', '0']), 0.)\\\n",
    "                                        .otherwise(100 * col('unpaidbalfirstsecond') / (expr_og_homevalue * col('hpi_change_exp')))\n",
    "                                )\n",
    "\n",
    "    expr_currentcltv = when(expr_currentcltv_cal.isNull(), 70.8)\\\n",
    "                    .otherwise(\n",
    "                        when(expr_currentcltv_cal > 150, 150)\\\n",
    "                            .otherwise(\n",
    "                                when(expr_currentcltv_cal < 10, 10)\\\n",
    "                                    .otherwise(expr_currentcltv_cal)\n",
    "                            )\n",
    "                    )\n",
    "    \n",
    "    expr_orig_fico = F.when(F.col('orig_fico').isNull(), 735)\\\n",
    "                            .otherwise(\n",
    "                                F.when(F.col('orig_fico').isin(['null', 'NULL']), 735)\\\n",
    "                                    .otherwise(\n",
    "                                        F.when(F.col('orig_fico') > 850, 850)\\\n",
    "                                            .otherwise(\n",
    "                                                F.when(F.col('orig_fico') < 500, 500)\\\n",
    "                                                    .otherwise(F.col('orig_fico').cast('int')\n",
    "                                        )\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "    \n",
    "    #Liveyears\n",
    "    expr_liveyears = F.when(F.col('lengthresidence').isNotNull(), F.col('lengthresidence'))\\\n",
    "                        .otherwise(\n",
    "                            F.when(F.col('liveyears').isNotNull(), F.col('liveyears'))\\\n",
    "                                .otherwise(expr_ageinmon/12)\n",
    "                            )\n",
    "    \n",
    "    expr_liveyears_short = F.when(expr_liveyears <= 4., expr_liveyears).otherwise(0.)\n",
    "    \n",
    "    expr_liveyears_long = F.when(expr_liveyears > 4, expr_liveyears).otherwise(0.)\n",
    "                                  \n",
    "    expr_liveyears_grp = F.when(expr_liveyears <= 4, 1.).otherwise(0.)                             \n",
    "        \n",
    "    expr_ln_purpose_type = F.when(F.col('ln_purpose_type').isin(['NULL', 'null', 'other']), 'refinance')\\\n",
    "                                .otherwise(F.lower(F.col('ln_purpose_type')))\n",
    "    \n",
    "    expr_og_occupy_stat_type = F.when(F.col('og_occupy_stat_type').isin(['NULL', 'null', 'other']), 'investment property')\\\n",
    "                            .otherwise(F.lower(F.col('og_occupy_stat_type')))\n",
    "\n",
    "    expr_issingleborrower = F.when(F.col('issingleborrower').isNull(), 0)\\\n",
    "                                .otherwise(\n",
    "                                    F.when(F.col('issingleborrower').isin(['NULL', 'null']), 0)\n",
    "                                        .otherwise(F.col('issingleborrower')))\n",
    "    \n",
    "    expr_ln_tr = F.when(F.col('ln_tr').isNull(), 360)\\\n",
    "                    .otherwise(\n",
    "                        F.when(F.col('ln_tr').isin(['NULL', 'null']), 360)\\\n",
    "                            .otherwise(F.col('ln_tr')))\n",
    "\n",
    "        \n",
    "    df = df.withColumn('ct_age_exp', expr_ct_age)\\\n",
    "        .withColumn('ct_1_exp', expr_ct_1_mo)\\\n",
    "        .withColumn('ct_2_exp', expr_ct_2_mo)\\\n",
    "        .withColumn('ct_3_exp', expr_ct_3_mo)\\\n",
    "        .withColumn('home_value_exp', expr_home_value)\\\n",
    "        .withColumn('p_31_50_exp', expr_age_31_50)\\\n",
    "        .withColumn('p_edu_hs_exp', expr_edu_hs)\\\n",
    "        .withColumn('p_married_exp', expr_married)\\\n",
    "        .withColumn('personnum_per_room_exp', expr_person_per_room)\\\n",
    "        .withColumn('mosaic_group_refi_exp', expr_mosaicgroup_refi_clust)\\\n",
    "        .withColumn('mosaic_group_pur_exp', expr_mosaicgroup_pur_clust)\\\n",
    "        .withColumn('ratespread_min_exp', expr_rate_spread_min)\\\n",
    "        .withColumn('ratespread_min_pur_exp', expr_rate_spread_min_pur)\\\n",
    "        .withColumn('borrowerage_bucket', expr_borrowerage_group)\\\n",
    "        .withColumn('loantypedescription_exp', expr_loan_type)\\\n",
    "        .withColumn('ageinmon_exp', expr_ageinmon)\\\n",
    "        .withColumn('og_mtg_am_exp', expr_og_mtg_am)\\\n",
    "        .withColumn('currentcltv_exp', expr_currentcltv)\\\n",
    "        .withColumn('orig_fico_exp', expr_orig_fico)\\\n",
    "        .withColumn('LiveYears_short_exp', expr_liveyears_short)\\\n",
    "        .withColumn('LiveYears_long_exp', expr_liveyears_long)\\\n",
    "        .withColumn('LiveYears_grp_exp', expr_liveyears_grp)\\\n",
    "        .withColumn('ln_purpose_type_exp', expr_ln_purpose_type)\\\n",
    "        .withColumn('og_occupy_stat_type_exp', expr_og_occupy_stat_type)\\\n",
    "        .withColumn('issingleborrower_exp', expr_issingleborrower)\\\n",
    "        .withColumn('ln_tr_exp', expr_ln_tr)\\\n",
    "        .withColumn('loanamortizationtype', F.lower(F.col('loanamortizationtype')))\\\n",
    "        .withColumn('ln_ann_int_rt', F.col('ln_ann_int_rt').cast('double'))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def save_preprocess(df, filename):\n",
    "    \n",
    "    df_prep = preprocess_dataset(df)\n",
    "    \n",
    "    df1 = df_prep.select('servicecalendardate', 'ln_no', 'og_note_dt',\n",
    "                           'ct_age_exp', 'ct_1_exp', 'ct_2_exp', 'ct_3_exp',\n",
    "                           'home_value_exp', 'p_31_50_exp', 'p_edu_hs_exp', 'p_married_exp',\n",
    "                           'personnum_per_room_exp', 'mosaic_group_refi_exp', 'mosaic_group_pur_exp',\n",
    "                           'ratespread_min_exp', 'ratespread_min_pur_exp', 'ln_ann_int_rt',\n",
    "                           'borrowerage_bucket', 'loantypedescription_exp', 'loanamortizationtype', 'ageinmon_exp',\n",
    "                           'og_mtg_am_exp', 'currentcltv_exp', 'orig_fico_exp',\n",
    "                           'LiveYears_short_exp', 'LiveYears_long_exp', 'LiveYears_grp_exp',\n",
    "                           'ln_purpose_type_exp', 'ln_purpose_type', 'og_occupy_stat_type_exp', 'issingleborrower_exp',\n",
    "                           'ln_tr_exp', 'investornameshort').drop_duplicates()\n",
    "    \n",
    "    df1.write.parquet(monthly_prep_path + filename, mode='overwrite')\n",
    "    \n",
    "# select('gcid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "save_preprocess(df_all, 'monthly_preprocessed_201909.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763391\n",
      "+-------------------+----------+-------------------+----------+--------+--------+--------+--------------+-----------+------------+-------------+----------------------+---------------------+--------------------+------------------+----------------------+-------------+------------------+-----------------------+--------------------+------------+-------------+-----------------+-------------+-------------------+------------------+-----------------+-------------------+---------------+-----------------------+--------------------+---------+-----------------+\n",
      "|servicecalendardate|     ln_no|         og_note_dt|ct_age_exp|ct_1_exp|ct_2_exp|ct_3_exp|home_value_exp|p_31_50_exp|p_edu_hs_exp|p_married_exp|personnum_per_room_exp|mosaic_group_refi_exp|mosaic_group_pur_exp|ratespread_min_exp|ratespread_min_pur_exp|ln_ann_int_rt|borrowerage_bucket|loantypedescription_exp|loanamortizationtype|ageinmon_exp|og_mtg_am_exp|  currentcltv_exp|orig_fico_exp|LiveYears_short_exp|LiveYears_long_exp|LiveYears_grp_exp|ln_purpose_type_exp|ln_purpose_type|og_occupy_stat_type_exp|issingleborrower_exp|ln_tr_exp|investornameshort|\n",
      "+-------------------+----------+-------------------+----------+--------+--------+--------+--------------+-----------+------------+-------------+----------------------+---------------------+--------------------+------------------+----------------------+-------------+------------------+-----------------------+--------------------+------------+-------------+-----------------+-------------+-------------------+------------------+-----------------+-------------------+---------------+-----------------------+--------------------+---------+-----------------+\n",
      "|         2019-09-30|3304745469|2012-01-10 00:00:00|         0|       0|       0|       0|      127000.0|          1|           0|            1|   0.16666666666666666|                  BHQ|               other|             2.202|                -2.575|        0.045|               old|                   conv|               fixed| 92.64516129|     153100.0|77.53421575662516|          655|                0.0|               0.0|              1.0|          refinance|      Refinance|      primary residence|                   1|      360|             FNMA|\n",
      "+-------------------+----------+-------------------+----------+--------+--------+--------+--------------+-----------+------------+-------------+----------------------+---------------------+--------------------+------------------+----------------------+-------------+------------------+-----------------------+--------------------+------------+-------------+-----------------+-------------+-------------------+------------------+-----------------+-------------------+---------------+-----------------------+--------------------+---------+-----------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "preprocessed_month_data = spark.read.parquet(monthly_prep_path + 'monthly_preprocessed_201909.parquet')\n",
    "\n",
    "print(preprocessed_month_data.count())\n",
    "preprocessed_month_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mover Refi Model Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Read Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "refi_model_pipeline_lr = PipelineModel.load(path = model_save_path_refi+'refiV1_3_LR_20190716')\n",
    "refi_model_pipeline_rf = PipelineModel.load(path = model_save_path_refi+'refiV1_3_RF_20190716')\n",
    "refi_model_pipeline_gbt = PipelineModel.load(path = model_save_path_refi+'refiV1_3_GBT_20190716')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Rename columns to score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "preprocessed_month_data_refi = preprocessed_month_data             \n",
    "    \n",
    "#.withColumnRenamed('ratespread_min_exp', 'ratespread_min_refi_exp')\n",
    "#.withColumnRenamed('mosaic_group_refi_exp', 'mosaic_group_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fullMonthPrediction_refi(df):\n",
    "    \n",
    "    _scoreUdf = F.udf(lambda v: float(v[1]), T.DoubleType())\n",
    "    \n",
    "    predictionsLR = refi_model_pipeline_lr.transform(df)\n",
    "    pred_df_lr = predictionsLR.withColumn('pred', _scoreUdf(predictionsLR['probability']))\\\n",
    "                                .select('ln_no', 'og_note_dt', 'investornameshort', 'loanamortizationtype', \n",
    "                                        col('loantypedescription_exp').alias('loantypedescription')\n",
    "                                        col('ln_tr_exp').alias('loan_term'), \n",
    "                                        col('ageinmon_exp').alias('loan_age'), \n",
    "                                        col('currentcltv_exp').alias('currentcltv'),\n",
    "                                        col('ln_ann_int_rt').alias('loan_interest_rate'), \n",
    "                                        col('pred').alias('logRegProb'))\n",
    "    \n",
    "    predictionsRF = refi_model_pipeline_rf.transform(df)\n",
    "    pred_df_rf = predictionsRF.withColumn('pred', _scoreUdf(predictionsRF['probability']))\\\n",
    "                               .select('ln_no', col('pred').alias('randForProb'))\n",
    "\n",
    "    predictionsGBT = refi_model_pipeline_gbt.transform(df)\n",
    "    pred_df_gbt = predictionsGBT.withColumn('pred', _scoreUdf(predictionsGBT['probability']))\\\n",
    "                                .select('ln_no', col('pred').alias('gbtProb'))\n",
    "\n",
    "    final_join = pred_df_lr.join(pred_df_rf, on='ln_no', how='left')\\\n",
    "                           .join(pred_df_gbt, on='ln_no', how='left')\n",
    "     \n",
    "    final_join = final_join.dropDuplicates()\n",
    "    \n",
    "    return final_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def dedup_result_pred_refi(pred_df):\n",
    "    \n",
    "    pred_df_dedup = pred_df.groupby(['ln_no', 'og_note_dt', 'loantypedescription', 'loanamortizationtype', 'investornameshort']).mean()\n",
    "    \n",
    "    oldcols = pred_df_dedup.schema.names\n",
    "    newcols = ['ln_no', 'og_note_dt', 'loantypedescription', 'loanamortizationtype', 'investornameshort',\\\n",
    "               'loan_term', 'loan_age', 'currentcltv', 'loan_interest_rate',\\\n",
    "               'logRegProb', 'randForProb', 'gbtProb']\n",
    "    pred_df_dedup = reduce(lambda pred_df_dedup, idx: pred_df_dedup.withColumnRenamed(oldcols[idx], newcols[idx]), range(len(oldcols)), pred_df_dedup)\n",
    "    \n",
    "    return pred_df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def save_csv_refi(df_prep, filename, result_path = result_path_refi):\n",
    "    \n",
    "    pred = fullMonthPrediction_refi(df_prep)\n",
    "    dedup = dedup_result_pred_refi(pred)\n",
    "    \n",
    "    dedup.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.1\n",
    "pred201801_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201801_dedup_refi = dedup_result_pred_refi(pred201801_refi)\\\n",
    "                            .select('ln_no', 'logRegProb', 'randForProb', 'gbtProb')\n",
    "\n",
    "save_results_csv_refi(pred201801_dedup_refi, 'pred_refiV1_jan18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.2\n",
    "pred201802_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201802_dedup_refi = dedup_result_pred_refi(pred201802_refi)\\\n",
    "                            .select('ln_no', 'logRegProb', 'randForProb', 'gbtProb')\n",
    "\n",
    "save_results_csv_refi(pred201802_dedup_refi, 'pred_refiV1_feb18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.4\n",
    "pred201804_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201804_dedup_refi = dedup_result_pred_refi(pred201804_refi)\\\n",
    "                            .select('ln_no', 'logRegProb', 'randForProb', 'gbtProb')\n",
    "\n",
    "save_results_csv_refi(pred201804_dedup_refi, 'pred_refiV1_apr18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.5\n",
    "pred201805_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201805_dedup_refi = dedup_result_pred_refi(pred201805_refi)\\\n",
    "                            .select('ln_no', 'logRegProb', 'randForProb', 'gbtProb')\n",
    "\n",
    "save_results_csv_refi(pred201805_dedup_refi, 'pred_refiV1_may18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.6\n",
    "pred201806_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201806_dedup_refi = dedup_result_pred_refi(pred201806_refi)\\\n",
    "                            .select('ln_no', 'logRegProb', 'randForProb', 'gbtProb')\n",
    "\n",
    "save_results_csv_refi(pred201806_dedup_refi, 'pred_refiV1_jun18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.7\n",
    "pred201807_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201807_dedup_refi = dedup_result_pred_refi(pred201807_refi)\\\n",
    "                            .select('ln_no', 'logRegProb', 'randForProb', 'gbtProb')\n",
    "\n",
    "save_results_csv_refi(pred201807_dedup_refi, 'pred_refiV1_jul18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.8\n",
    "pred201808_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201808_dedup_refi = dedup_result_pred_refi(pred201808_refi)\\\n",
    "                            .select('ln_no', 'logRegProb', 'randForProb', 'gbtProb')\n",
    "\n",
    "save_results_csv_refi(pred201808_dedup_refi, 'pred_refiV1_aug18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.12\n",
    "pred201812_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201812_dedup_refi = dedup_result_pred_refi(pred201812_refi)\n",
    "\n",
    "save_results_csv_refi(pred201812_dedup_refi, 'pred_refiV1_dec18_20190205.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.1\n",
    "pred201901_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201901_dedup_refi = dedup_result_pred_refi(pred201901_refi)\n",
    "\n",
    "save_results_csv_refi(pred201901_dedup_refi, 'pred_refiV1_jan19_20190215.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.2\n",
    "pred201902_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201902_dedup_refi = dedup_result_pred_refi(pred201902_refi)\n",
    "\n",
    "save_results_csv_refi(pred201902_dedup_refi, 'pred_refiV1_feb19_20190305.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.3\n",
    "pred201903_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201903_dedup_refi = dedup_result_pred_refi(pred201903_refi)\n",
    "\n",
    "save_results_csv_refi(pred201903_dedup_refi, 'pred_refiV1_mar19_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.4\n",
    "pred201904_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201904_dedup_refi = dedup_result_pred_refi(pred201904_refi)\n",
    "\n",
    "save_results_csv_refi(pred201904_dedup_refi, 'pred_refiV1_apr19_20190507.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.5\n",
    "pred201905_refi = fullMonthPrediction_refi(preprocessed_month_data_refi)\n",
    "pred201905_dedup_refi = dedup_result_pred_refi(pred201905_refi)\n",
    "\n",
    "save_results_csv_refi(pred201905_dedup_refi, 'pred_refiV1_2_may19_20190610.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.6\n",
    "save_csv_refi(preprocessed_month_data_refi, 'pred_refiV1_3_jun19_20190717.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.7\n",
    "save_csv_refi(preprocessed_month_data_refi, 'pred_refiV1_3_jul19_20190805.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.8\n",
    "save_csv_refi(preprocessed_month_data_refi, 'pred_refiV1_3_aug19_20190904.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.9\n",
    "save_csv_refi(preprocessed_month_data_refi, 'pred_refiV1_3_sep19_20191004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mover Purchase Model Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Read Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "pur_model_pipeline_lr = PipelineModel.load(path = model_save_path_pur + 'MoverV3_2_LR_20190718')\n",
    "pur_model_pipeline_rf = PipelineModel.load(path = model_save_path_pur + 'MoverV3_2_RF_20190718')\n",
    "pur_model_pipeline_gbt = PipelineModel.load(path = model_save_path_pur + 'MoverV3_2_GBT_20190718')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Rename columns to score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "preprocessed_month_data_pur = preprocessed_month_data\n",
    "\n",
    "#.withColumn('loantype_exp', F.upper(F.col('loantypedescription_exp')))\\\n",
    "#.withColumnRenamed('ratespread_min_pur_exp', 'ratecompare_exp')\\\n",
    "#.withColumnRenamed('mosaic_group_pur_exp', 'mosaic_group_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fullMonthPrediction_pur(df):\n",
    "    \n",
    "    _scoreUdf = F.udf(lambda v: float(v[1]), T.DoubleType())\n",
    "    \n",
    "    predictionsLR = pur_model_pipeline_lr.transform(df)\n",
    "    pred_df_lr = predictionsLR.withColumn('pred', _scoreUdf(predictionsLR['probability']))\\\n",
    "                                .select('ln_no', col('pred').alias('logRegProb'))\n",
    "    \n",
    "    predictionsRF = pur_model_pipeline_rf.transform(df)\n",
    "    pred_df_rf = predictionsRF.withColumn('pred', _scoreUdf(predictionsRF['probability']))\\\n",
    "                               .select('ln_no', col('pred').alias('randForProb'))\n",
    "\n",
    "    predictionsGBT = pur_model_pipeline_gbt.transform(df)\n",
    "    pred_df_gbt = predictionsGBT.withColumn('pred', _scoreUdf(predictionsGBT['probability']))\\\n",
    "                                .select('ln_no', col('pred').alias('gbtProb'))\n",
    "\n",
    "    final_join = pred_df_lr.join(pred_df_rf, on='ln_no', how='left')\\\n",
    "                           .join(pred_df_gbt, on='ln_no', how='left')\n",
    "     \n",
    "    final_join = final_join.dropDuplicates()\n",
    "    \n",
    "    return final_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def dedup_result_pred_pur(pred_df):\n",
    "    \n",
    "    pred_df_dedup = pred_df.groupby('ln_no')\\\n",
    "                            .agg(F.avg('logRegProb').alias('logRegProb'),\n",
    "                                 F.avg('randForProb').alias('randForProb'),\n",
    "                                 F.avg('gbtProb').alias('gbtProb')\n",
    "                            )\n",
    "\n",
    "    return pred_df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def save_csv_pur(df_prep, filename, result_path = result_path_pur):\n",
    "    \n",
    "    pred = fullMonthPrediction_pur(df_prep)\n",
    "    dedup = dedup_result_pred_pur(pred)\n",
    "    \n",
    "    dedup.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.1\n",
    "pred201801_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201801_dedup_pur = dedup_result_pred_pur(pred201801_pur)\n",
    "\n",
    "save_results_csv_pur(pred201801_dedup_pur, 'pred_Mover_CT3_jan18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.2\n",
    "pred201802_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201802_dedup_pur = dedup_result_pred_pur(pred201802_pur)\n",
    "\n",
    "save_results_csv_pur(pred201802_dedup_pur, 'pred_Mover_CT3_feb18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.4\n",
    "pred201804_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201804_dedup_pur = dedup_result_pred_pur(pred201804_pur)\n",
    "\n",
    "save_results_csv_pur(pred201804_dedup_pur, 'pred_Mover_CT3_apr18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.5\n",
    "pred201805_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201805_dedup_pur = dedup_result_pred_pur(pred201805_pur)\n",
    "\n",
    "save_results_csv_pur(pred201805_dedup_pur, 'pred_Mover_CT3_may18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.11\n",
    "pred201811_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201811_dedup_pur = dedup_result_pred_pur(pred201811_pur)\n",
    "\n",
    "save_results_csv_pur(pred201811_dedup_pur, 'pred_Mover_CT3_nov18_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2018.12\n",
    "pred201812_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201812_dedup_pur = dedup_result_pred_pur(pred201812_pur)\n",
    "\n",
    "save_results_csv_pur(pred201812_dedup_pur, 'pred_Mover_CT3_dec18_20190131.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.1\n",
    "pred201901_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201901_dedup_pur = dedup_result_pred_pur(pred201901_pur)\n",
    "\n",
    "save_results_csv_pur(pred201901_dedup_pur, 'pred_Mover_CT3_jan19_20190215.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.2\n",
    "pred201902_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201902_dedup_pur = dedup_result_pred_pur(pred201902_pur)\n",
    "\n",
    "save_results_csv_pur(pred201902_dedup_pur, 'pred_Mover_CT3_feb19_20190305.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.3\n",
    "pred201903_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201903_dedup_pur = dedup_result_pred_pur(pred201903_pur)\n",
    "\n",
    "save_results_csv_pur(pred201903_dedup_pur, 'pred_Mover_CT3_mar19_20190409.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.4\n",
    "pred201904_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201904_dedup_pur = dedup_result_pred_pur(pred201904_pur)\n",
    "\n",
    "save_results_csv_pur(pred201904_dedup_pur, 'pred_Mover_CT3_apr19_20190507.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.5\n",
    "pred201905_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201905_dedup_pur = dedup_result_pred_pur(pred201905_pur)\n",
    "\n",
    "save_results_csv_pur(pred201905_dedup_pur, 'pred_Mover_CT3_may19_20190610.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.6\n",
    "pred201906_pur = fullMonthPrediction_pur(preprocessed_month_data_pur)\n",
    "pred201906_dedup_pur = dedup_result_pred_pur(pred201906_pur)\n",
    "\n",
    "save_results_csv_pur(pred201906_dedup_pur, 'pred_Mover_CT3_june19_20190708.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.7\n",
    "save_csv_pur(preprocessed_month_data_pur, 'pred_MoverV3_2_jul19_20190805.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.8\n",
    "save_csv_pur(preprocessed_month_data_pur, 'pred_MoverV3_2_aug19_20190904.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#2019.9\n",
    "save_csv_pur(preprocessed_month_data_pur, 'pred_MoverV3_2_sep19_20191004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Expanded Mover Purchase Models Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1). Read Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 3-6 Months \n",
    "model_pipeline_lr_6 = PipelineModel.load(path = model_save_path_pur + 'Mover_3to6_V1_LR_20190719')\n",
    "model_pipeline_rf_6 = PipelineModel.load(path = model_save_path_pur + 'Mover_3to6_V1_RF_20190719')\n",
    "model_pipeline_gbt_6 = PipelineModel.load(path = model_save_path_pur + 'Mover_3to6_V1_GBT_20190719')\n",
    "\n",
    "# 6-12 Months\n",
    "model_pipeline_lr_12 = PipelineModel.load(path = model_save_path_pur + 'Mover_6to12_V1_LR_20190719')\n",
    "model_pipeline_rf_12 = PipelineModel.load(path = model_save_path_pur + 'Mover_6to12_V1_RF_20190719')\n",
    "model_pipeline_gbt_12 = PipelineModel.load(path = model_save_path_pur + 'Mover_6to12_V1_GBT_20190719')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2). Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- servicecalendardate: date (nullable = true)\n",
      " |-- ln_no: string (nullable = true)\n",
      " |-- og_note_dt: timestamp (nullable = true)\n",
      " |-- ct_age_exp: integer (nullable = true)\n",
      " |-- ct_1_exp: string (nullable = true)\n",
      " |-- ct_2_exp: string (nullable = true)\n",
      " |-- ct_3_exp: string (nullable = true)\n",
      " |-- home_value_exp: double (nullable = true)\n",
      " |-- p_31_50_exp: integer (nullable = true)\n",
      " |-- p_edu_hs_exp: integer (nullable = true)\n",
      " |-- p_married_exp: integer (nullable = true)\n",
      " |-- personnum_per_room_exp: double (nullable = true)\n",
      " |-- mosaic_group_refi_exp: string (nullable = true)\n",
      " |-- mosaic_group_pur_exp: string (nullable = true)\n",
      " |-- ratespread_min_exp: double (nullable = true)\n",
      " |-- ratespread_min_pur_exp: double (nullable = true)\n",
      " |-- ln_ann_int_rt: double (nullable = true)\n",
      " |-- borrowerage_bucket: string (nullable = true)\n",
      " |-- loantypedescription_exp: string (nullable = true)\n",
      " |-- loanamortizationtype: string (nullable = true)\n",
      " |-- ageinmon_exp: double (nullable = true)\n",
      " |-- og_mtg_am_exp: double (nullable = true)\n",
      " |-- currentcltv_exp: double (nullable = true)\n",
      " |-- orig_fico_exp: integer (nullable = true)\n",
      " |-- LiveYears_short_exp: double (nullable = true)\n",
      " |-- LiveYears_long_exp: double (nullable = true)\n",
      " |-- LiveYears_grp_exp: double (nullable = true)\n",
      " |-- ln_purpose_type_exp: string (nullable = true)\n",
      " |-- ln_purpose_type: string (nullable = true)\n",
      " |-- og_occupy_stat_type_exp: string (nullable = true)\n",
      " |-- issingleborrower_exp: integer (nullable = true)\n",
      " |-- ln_tr_exp: integer (nullable = true)\n",
      " |-- investornameshort: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "preprocessed_month_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def predict(model_pipeline, data):\n",
    "    \n",
    "    _scoreUdf = udf(lambda v: float(v[1]), DoubleType())\n",
    "    \n",
    "    prediction = model_pipeline.transform(data)\n",
    "    pred_df = prediction.withColumn('pred', _scoreUdf(prediction['probability']))\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def fullMonthPrediction_pur_ex(df, model_pipeline_lr, model_pipeline_rf, model_pipeline_gbt):\n",
    "    \n",
    "    _scoreUdf = F.udf(lambda v: float(v[1]), T.DoubleType())\n",
    "    \n",
    "    predictionsLR = model_pipeline_lr.transform(df)\n",
    "    pred_df_lr = predictionsLR.withColumn('pred', _scoreUdf(predictionsLR['probability']))\\\n",
    "                                .select('ln_no', 'og_note_dt', col('pred').alias('logRegProb'))\n",
    "    \n",
    "    predictionsRF = model_pipeline_rf.transform(df)\n",
    "    pred_df_rf = predictionsRF.withColumn('pred', _scoreUdf(predictionsRF['probability']))\\\n",
    "                                .select('ln_no', col('pred').alias('randForProb'))\n",
    "    \n",
    "    predictionsGBT = model_pipeline_gbt.transform(df)\n",
    "    pred_df_gbt = predictionsGBT.withColumn('pred', _scoreUdf(predictionsGBT['probability']))\\\n",
    "                                .select('ln_no', col('pred').alias('gbtProb'))\n",
    "    \n",
    "    combinedFinalPred = pred_df_lr.join(pred_df_rf, on='ln_no', how='left')\\\n",
    "                                    .join(pred_df_gbt, on='ln_no', how='left')\\\n",
    "                                    .dropDuplicates()\n",
    "    \n",
    "    return combinedFinalPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def dedup_result_pred_pur_ex(pred_df):\n",
    "    \n",
    "    pred_df_dedup = pred_df.groupby('ln_no', 'og_note_dt')\\\n",
    "                            .agg(F.avg('logRegProb').alias('logRegProb'),\n",
    "                                 F.avg('randForProb').alias('randForProb'),\n",
    "                                 F.avg('gbtProb').alias('gbtProb')\n",
    "                            )\n",
    "\n",
    "    return pred_df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def save_csv_pur(df_prep, model_pipeline_lr, model_pipeline_rf, model_pipeline_gbt, filename, result_path):\n",
    "    \n",
    "    pred = fullMonthPrediction_pur_ex(df_prep, model_pipeline_lr, model_pipeline_rf, model_pipeline_gbt)\n",
    "    dedup = dedup_result_pred_pur_ex(pred)\n",
    "    \n",
    "    dedup.coalesce(1).write.csv(result_path + filename, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3). 3-6 Months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 2019-9\n",
    "save_csv_pur(preprocessed_month_data, \n",
    "             model_pipeline_lr_6, model_pipeline_rf_6, model_pipeline_gbt_6,\n",
    "             'pred_Mover_3to6_V1_sep19_20191018.csv', \n",
    "             result_path_pur_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (4). 6-12 Months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# 2019-9\n",
    "save_csv_pur(preprocessed_month_data, \n",
    "             model_pipeline_lr_12, model_pipeline_rf_12, model_pipeline_gbt_12,\n",
    "             'pred_Mover_6to12_V1_sep19_20191018.csv', \n",
    "             result_path_pur_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7. Compile Mover Purchase Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1). Compile Monthly Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def compile_scores(scorefile_3, scorefile_6, scorefile_12):\n",
    "    \n",
    "    df_gcid = spark.sql('select * from data_science_sandbox.gcidservicingloans')\n",
    "    \n",
    "    score_3 = spark.read.csv(result_path_pur + scorefile_3, header=True)\\\n",
    "                    .select('ln_no', \n",
    "                            col('gbtProb').cast('double').alias('score_0_3mo'))\n",
    "    score_6 = spark.read.csv(result_path_pur_6 + scorefile_6, header=True)\\\n",
    "                    .select('ln_no', \n",
    "                            col('og_note_dt').substr(1, 10).alias('origination_date'),\n",
    "                            col('gbtProb').cast('double').alias('score_3_6mo'))\n",
    "    score_12 = spark.read.csv(result_path_pur_12 + scorefile_12, header=True)\\\n",
    "                    .select('ln_no', col('gbtProb').cast('double').alias('score_6_12mo'))\n",
    "        \n",
    "    score_all = score_3.join(score_6, on='ln_no', how='inner')\\\n",
    "                        .join(score_12, on='ln_no', how='inner')\n",
    "        \n",
    "    ls = []\n",
    "    for score in ['score_0_3mo', 'score_3_6mo', 'score_6_12mo']:\n",
    "        ls.extend(statFunc(score_all).approxQuantile(score, [0.7, 0.85, 0.95], 0.00001))\n",
    "        \n",
    "    # Top 5% of score_0_3mo are Mover\n",
    "    # Remaining of top 15% of score_3_6mo are Searcher (~10%)\n",
    "    # Remaining of top 30% of score_6_12mo are Thinker (~15%)\n",
    "    expr_type = when(col('score_0_3mo') >= ls[2], 'Mover')\\\n",
    "                    .otherwise(\n",
    "                        when(col('score_3_6mo') >= ls[4], 'Searcher')\\\n",
    "                            .otherwise(\n",
    "                                when(col('score_6_12mo') >= ls[6], 'Thinker')\\\n",
    "                                    .otherwise('Other')\n",
    "                            )\n",
    "                    )\n",
    "\n",
    "    score_all = score_all.withColumn('type', expr_type)\\\n",
    "                            .join(df_gcid, score_all.ln_no == df_gcid.Loannumber, how='left')\n",
    "    \n",
    "    return score_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "def score_save(scorefile_3, scorefile_6, scorefile_12, servicedate, filename):\n",
    "    \n",
    "    score_df = compile_scores(scorefile_3, scorefile_6, scorefile_12)\n",
    "    \n",
    "    df1 = score_df.withColumn('servicecalendardate', lit(servicedate))\\\n",
    "                    .select('servicecalendardate', 'ln_no', 'GCID', 'origination_date', \n",
    "                            'type', 'score_0_3mo', 'score_3_6mo', 'score_6_12mo')\n",
    "    \n",
    "    df1.coalesce(1).write.csv(result_path_pur_all + filename, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "score_save(\n",
    "    scorefile_3 = 'pred_MoverV3_2_sep19_20191004.csv',\n",
    "    scorefile_6 = 'pred_Mover_3to6_V1_sep19_20191018.csv',\n",
    "    scorefile_12 = 'pred_Mover_6to12_V1_sep19_20191018.csv',\n",
    "    servicedate = '2019-09-30',\n",
    "    filename = 'pred_Mover_Pur_All_sep19_20191018.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2). Insert into the table for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763391\n",
      "+-------------------+----------+--------+----------------+-----+------------------+------------------+-------------------+\n",
      "|servicecalendardate|     ln_no|    GCID|origination_date| type|       score_0_3mo|       score_3_6mo|       score_6_12mo|\n",
      "+-------------------+----------+--------+----------------+-----+------------------+------------------+-------------------+\n",
      "|         2019-09-30|3221449353|15625598|      2009-10-13|Mover|0.6966082344170185|0.4048104830014989|0.38521164712275213|\n",
      "+-------------------+----------+--------+----------------+-----+------------------+------------------+-------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "t1 = spark.read.csv(result_path_pur_all + 'pred_Mover_Pur_All_sep19_20191018.csv', header=True)\n",
    "\n",
    "print(t1.count())\n",
    "t1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    type|  count|\n",
      "+--------+-------+\n",
      "|Searcher| 197888|\n",
      "| Thinker| 288212|\n",
      "|   Other|1189124|\n",
      "|   Mover|  88167|\n",
      "+--------+-------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "t1.groupby('type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# First snapshot: 9/30/2019\n",
    "# Going forward, insert into the table, instead of overwriting.\n",
    "\n",
    "t1.write.mode('overwrite').saveAsTable('Analytical.wol_dashboard_serviced_purchase_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8. Save Results into Prod Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1). Save Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763391\n",
      "1763391\n",
      "+----------+------------------+\n",
      "|      loan|             score|\n",
      "+----------+------------------+\n",
      "|3349145024|0.4837531658039408|\n",
      "+----------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------------+\n",
      "|      loan|              score|\n",
      "+----------+-------------------+\n",
      "|3418610851|0.09029954693434494|\n",
      "+----------+-------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "score_refi = spark.read.csv(result_path_refi + 'pred_refiV1_3_sep19_20191004.csv', header=True)\\\n",
    "                    .select(col('ln_no').alias('loan'), col('gbtProb').alias('score'))\n",
    "        \n",
    "score_pur = spark.read.csv(result_path_pur + 'pred_MoverV3_2_sep19_20191004.csv', header=True)\\\n",
    "                    .select(col('ln_no').alias('loan'), col('gbtProb').alias('score'))\n",
    "\n",
    "print(score_refi.count())\n",
    "print(score_pur.count())\n",
    "\n",
    "score_refi.show(1)\n",
    "score_pur.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "score_refi.coalesce(1).write.csv(prod_refi_path + 'leadgen_servicing_mover_refi_20190930.csv', header=True)\n",
    "score_pur.coalesce(1).write.csv(prod_pur_path + 'leadgen_servicing_mover_pur_20190930.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2). Save score path, and update with new version model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|filepath                                                                                                      |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|/prod/projects/model_conformed/RocketScience/LeadGen/RefiLeadGen/1.0/leadgen_servicing_mover_refi_20190930.csv|\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|filepath                                                                                                    |\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|/prod/projects/model_conformed/RocketScience/LeadGen/PurLeadGen/1.0/leadgen_servicing_mover_pur_20190930.csv|\n",
      "+------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "refi_file = spark.createDataFrame([('/prod/projects/model_conformed/RocketScience/LeadGen/RefiLeadGen/1.0/leadgen_servicing_mover_refi_20190930.csv',)], ['filepath'])\n",
    "pur_file = spark.createDataFrame([('/prod/projects/model_conformed/RocketScience/LeadGen/PurLeadGen/1.0/leadgen_servicing_mover_pur_20190930.csv',)], ['filepath'])\n",
    "\n",
    "refi_file.show(10, False)\n",
    "pur_file.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# Before running, delete the two csv files in Ambari.\n",
    "#file_path = '/prod/projects/model_conformed/RocketScience/LeadGen/Currentfile/'\n",
    "\n",
    "refi_file.coalesce(1).write.csv(file_path + 'current_leadgen_servicing_mover_refi.csv', header=True)\n",
    "pur_file.coalesce(1).write.csv(file_path + 'current_leadgen_servicing_mover_pur.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
