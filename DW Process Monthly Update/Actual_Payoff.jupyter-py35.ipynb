{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success configuring sparkmagic livy.\n",
      "['https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1', 'https://qlawsdl001038a.ad.datalake.foc.zone:8443/gateway/dsx/livy/v1']\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "from dsx_core_utils import proxy_util,dsxhi_util\n",
    "proxy_util.configure_proxy_livy()\n",
    "dsxhi_util.list_livy_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\"executorCores\": 4, \"numExecutors\": 5, \"executorMemory\": \"10g\", \n",
    " \"driverMemory\": \"8g\", \"proxyUser\": \"jchen-\", \"driverCores\": 1, \n",
    " \"conf\": {\"spark.yarn.appMasterEnv.THEANO_FLAGS\": \"base_compiledir=${PWD}/.theano\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>21772</td><td>application_1590030838276_35322</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://qlawsbidlhm02b.ad.datalake.foc.zone:8090/proxy/application_1590030838276_35322/\">Link</a></td><td><a target=\"_blank\" href=\"https://qlawsbidlhw07a.ad.datalake.foc.zone:8042/node/containerlogs/container_e499_1590030838276_35322_01_000001/aliu-\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s servicingdata -k -l python -u https://qlawsbidlhe02a.ad.datalake.foc.zone:8445/gateway/dsx/livy2/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "import pyspark\n",
    "import os, sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when, lit, countDistinct, max, min, desc\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "import pyspark.sql.types as T\n",
    "from functools import reduce\n",
    "\n",
    "from os.path import expanduser, join, abspath\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2.3.0.2.6.5.0-292'"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "csv_path = '/dev/projects/retention_models/csv_data/'\n",
    "actual_path = '/dev/projects/retention_models/actual_value/'\n",
    "monthly_path = '/dev/projects/retention_models/monthly_snapshot/'\n",
    "\n",
    "past_path = '/dev/projects/retention_models/purchase_payoff/training/results/movermodel/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    " \n",
    "df_actual = spark.read.csv(csv_path + 'Payoff_segments_summary_20200610.csv', header=True)\\\n",
    "                        .withColumn('newloanpurpose', lower(col('NewLoanPurpose')))\\\n",
    "                        .withColumn('retentiontype', lower(col('PayoffRetentionType')))\\\n",
    "                        .select('ServicedLoanNumber', 'PaymentInFullDate', 'newloanpurpose', 'retentiontype')\n",
    "    \n",
    "df_list1 = spark.sql('''\n",
    "select distinct servicecalendardate, ln_no\n",
    "from data_science_sandbox.servicing_dbo_payoff_source_purchase_client_crdtrs \n",
    "where leftportfoliodate is null\n",
    "and servicecalendardate between '2016-06-30' and '2018-11-30'\n",
    "''')\n",
    "\n",
    "## upload from DW table\n",
    "## add more snapshot to update the current servicing loans payoff information\n",
    "\n",
    "df_201812 = spark.read.parquet(monthly_path+'servicing_df_all_dec18_20190131.parquet').select('servicecalendardate', 'ln_no')\n",
    " \n",
    "df_201901 = spark.read.parquet(monthly_path+'servicing_df_all_jan19_20190215.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201902 = spark.read.parquet(monthly_path+'servicing_df_all_feb19_20190305.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201903 = spark.read.parquet(monthly_path+'servicing_df_all_mar19_20190409.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201904 = spark.read.parquet(monthly_path+'servicing_df_all_apr19_20190507.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201905 = spark.read.parquet(monthly_path+'servicing_df_all_may19_20190610.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201906 = spark.read.parquet(monthly_path+'servicing_df_all_june19_20190708.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201907 = spark.read.parquet(monthly_path+'servicing_df_all_jul19_20190805.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201908 = spark.read.parquet(monthly_path+'servicing_df_all_aug19_20191226.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201909 = spark.read.parquet(monthly_path+'servicing_df_all_sep19_20191004.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201910 = spark.read.parquet(monthly_path+'servicing_df_all_oct19_20191105.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201911 = spark.read.parquet(monthly_path+'servicing_df_all_nov19_20191202.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_201912 = spark.read.parquet(monthly_path+'servicing_df_all_dec19_20200103.parquet').select('servicecalendardate', 'ln_no')\n",
    "\n",
    "df_202001 = spark.read.parquet(monthly_path+'servicing_df_all_jan20_20200203.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_202002 = spark.read.parquet(monthly_path+'servicing_df_all_feb20_20200305.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_202003 = spark.read.parquet(monthly_path+'servicing_df_all_mar20_20200406.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_202004 = spark.read.parquet(monthly_path+'servicing_df_all_apr20_20200505.parquet').select('servicecalendardate', 'ln_no')\n",
    "df_202005 = spark.read.parquet(monthly_path+'servicing_df_all_may20_20200610.parquet').select('servicecalendardate', 'ln_no')\n",
    "\n",
    "\n",
    "    \n",
    "## join all snapshot servicing lists\n",
    "df_list = df_list1.union(df_201812)\\\n",
    "                    .union(df_201901).union(df_201902).union(df_201903)\\\n",
    "                    .union(df_201904).union(df_201905).union(df_201906)\\\n",
    "                    .union(df_201907).union(df_201908).union(df_201909)\\\n",
    "                    .union(df_201910).union(df_201911).union(df_201912)\\\n",
    "                    .union(df_202001).union(df_202002).union(df_202003)\\\n",
    "                    .union(df_202004).union(df_202005)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|servicecalendardate|  count|\n",
      "+-------------------+-------+\n",
      "|         2020-05-31|1889579|\n",
      "|         2020-04-30|1899099|\n",
      "|         2020-03-31|1867297|\n",
      "|         2020-02-29|1846864|\n",
      "|         2020-01-31|1824706|\n",
      "|         2019-12-31|1802243|\n",
      "|         2019-11-30|1807991|\n",
      "|         2019-10-31|1787277|\n",
      "|         2019-09-30|1763391|\n",
      "|         2019-08-31|1856202|\n",
      "|         2019-07-31|1839005|\n",
      "|         2019-06-30|1822001|\n",
      "|         2019-05-31|1806604|\n",
      "|         2019-04-30|1788444|\n",
      "|         2019-03-31|1770015|\n",
      "|         2019-02-28|1752593|\n",
      "|         2019-01-31|1738794|\n",
      "|         2018-12-31|1726017|\n",
      "|         2018-11-30|1713615|\n",
      "|         2018-10-31|1699510|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "## check the total count\n",
    "df_list.groupBy('servicecalendardate').count().orderBy(desc('servicecalendardate')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_payoff = df_list.join(df_actual, df_actual.ServicedLoanNumber == df_list.ln_no, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "## create flags\n",
    "\n",
    "def build_targets(df):\n",
    "    \n",
    "    # Purchase Payoff\n",
    "    expr_purpayoff = when((col('PaymentInFullDate')>col('startdt')) & (col('PaymentInFullDate')<=col('enddt_3')) & (col('newloanpurpose')=='purchase'), 1.)\\\n",
    "                        .otherwise(0.)\n",
    "    expr_purpayoff_3_6 = when((col('PaymentInFullDate')>col('enddt_3')) & (col('PaymentInFullDate')<=col('enddt_6')) & (col('newloanpurpose')=='purchase'), 1.)\\\n",
    "                            .otherwise(0.)\n",
    "    expr_purpayoff_6_12 = when((col('PaymentInFullDate')>col('enddt_6')) & (col('PaymentInFullDate')<=col('enddt_12')) & (col('newloanpurpose')=='purchase'), 1.)\\\n",
    "                            .otherwise(0.)\n",
    "    expr_purpayoff_all = when((col('PaymentInFullDate')>col('servicecalendardate')) & (col('newloanpurpose')=='purchase'), 1.)\\\n",
    "                            .otherwise(0.)\n",
    "    \n",
    "    # Refi Payoff\n",
    "    expr_refipayoff = when((col('PaymentInFullDate')>col('startdt')) & (col('PaymentInFullDate')<=col('enddt_3')) & (col('newloanpurpose')!='purchase'), 1.)\\\n",
    "                        .otherwise(0.)\n",
    "    expr_refipayoff_all = when((col('PaymentInFullDate')>col('servicecalendardate')) & (col('newloanpurpose')!='purchase'), 1.)\\\n",
    "                            .otherwise(0.)\n",
    "    \n",
    "    # Purchase/Refi Retained\n",
    "    expr_purretain_24 = when((col('PaymentInFullDate')>col('startdt')) & (col('PaymentInFullDate')<=col('enddt_24')) \n",
    "                             & (col('newloanpurpose')=='purchase') & (col('retentiontype')=='retained'), 1.)\\\n",
    "                            .otherwise(0.)\n",
    "    \n",
    "    expr_refiretain_24 = when((col('PaymentInFullDate')>col('startdt')) & (col('PaymentInFullDate')<=col('enddt_24')) \n",
    "                             & (col('newloanpurpose')!='purchase') & (col('retentiontype')=='retained'), 1.)\\\n",
    "                            .otherwise(0.)\n",
    "    \n",
    "    # Payoff\n",
    "    expr_payoff = when((col('PaymentInFullDate')>col('startdt')) & (col('PaymentInFullDate')<=col('enddt_3')), 1.)\\\n",
    "                        .otherwise(0.)\n",
    "    expr_payoff_24 = when((col('PaymentInFullDate')>col('startdt')) & (col('PaymentInFullDate')<=col('enddt_24')), 1.)\\\n",
    "                        .otherwise(0.)\n",
    "    expr_payoff_all = when(col('PaymentInFullDate')>col('servicecalendardate'), 1.)\\\n",
    "                        .otherwise(0.)\n",
    "    \n",
    "    # Retained\n",
    "    expr_retention = when((col('PaymentInFullDate')>col('startdt')) & (col('PaymentInFullDate')<=col('enddt_3')) & (col('retentiontype')=='retained'), 1.)\\\n",
    "                        .otherwise(0.)\n",
    "    expr_retention_3_6 = when((col('PaymentInFullDate')>col('enddt_3')) & (col('PaymentInFullDate')<=col('enddt_6')) & (col('retentiontype')=='retained'), 1.)\\\n",
    "                            .otherwise(0.)\n",
    "    expr_retention_6_12 = when((col('PaymentInFullDate')>col('enddt_6')) & (col('PaymentInFullDate')<=col('enddt_12')) & (col('retentiontype')=='retained'), 1.)\\\n",
    "                            .otherwise(0.)\n",
    "    expr_retention_24 = when((col('PaymentInFullDate')>col('startdt')) & (col('PaymentInFullDate')<=col('enddt_24')) & (col('retentiontype')=='retained'), 1.)\\\n",
    "                        .otherwise(0.)\n",
    "    expr_retention_all = when((col('PaymentInFullDate')>col('servicecalendardate')) & (col('retentiontype')=='retained'), 1.)\\\n",
    "                        .otherwise(0.)\n",
    "        \n",
    "    ## the 05-31 snapshot predicts 06 propensity\n",
    "    ## when calculate actual payoff, need to start from 06-01 to get the payoffs\n",
    "    ## and in this logic, payoffs are counted from 07-01\n",
    "    df_final = df.withColumn('startdt', F.add_months(col('servicecalendardate'), 1))\\\n",
    "                    .withColumn('enddt_3', F.add_months(col('servicecalendardate'), 4))\\\n",
    "                    .withColumn('enddt_6', F.add_months(col('servicecalendardate'), 7))\\\n",
    "                    .withColumn('enddt_12', F.add_months(col('servicecalendardate'), 13))\\\n",
    "                    .withColumn('enddt_24', F.add_months(col('servicecalendardate'), 25))\\\n",
    "                    .withColumn('purchasepayoff', expr_purpayoff)\\\n",
    "                    .withColumn('purchasepayoff_3_6', expr_purpayoff_3_6)\\\n",
    "                    .withColumn('purchasepayoff_6_12', expr_purpayoff_6_12)\\\n",
    "                    .withColumn('purchasepayoff_all', expr_purpayoff_all)\\\n",
    "                    .withColumn('refipayoff', expr_refipayoff)\\\n",
    "                    .withColumn('refipayoff_all', expr_refipayoff_all)\\\n",
    "                    .withColumn('purretain_24', expr_purretain_24)\\\n",
    "                    .withColumn('refiretain_24', expr_refiretain_24)\\\n",
    "                    .withColumn('payoff', expr_payoff)\\\n",
    "                    .withColumn('payoff_24', expr_payoff_24)\\\n",
    "                    .withColumn('payoff_all', expr_payoff_all)\\\n",
    "                    .withColumn('retained', expr_retention)\\\n",
    "                    .withColumn('retained_3_6', expr_retention_3_6)\\\n",
    "                    .withColumn('retained_6_12', expr_retention_6_12)\\\n",
    "                    .withColumn('retained_24', expr_retention_24)\\\n",
    "                    .withColumn('retained_all', expr_retention_all)\\\n",
    "                    .select('servicecalendardate', 'ln_no', 'PaymentInFullDate', 'newloanpurpose', 'retentiontype',\n",
    "                            'purchasepayoff', 'purchasepayoff_3_6', 'purchasepayoff_6_12', 'purchasepayoff_all',\n",
    "                            'refipayoff', 'refipayoff_all', \n",
    "                            'purretain_24', 'refiretain_24',\n",
    "                            'payoff', 'payoff_24', 'payoff_all', \n",
    "                            'retained', 'retained_3_6', 'retained_6_12', 'retained_24', 'retained_all')\n",
    "                        \n",
    "    return df_final\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|servicecalendardate|  count|\n",
      "+-------------------+-------+\n",
      "|         2020-05-31|1889579|\n",
      "|         2020-04-30|1899099|\n",
      "|         2020-03-31|1867297|\n",
      "|         2020-02-29|1846864|\n",
      "|         2020-01-31|1824706|\n",
      "|         2019-12-31|1802243|\n",
      "|         2019-11-30|1807991|\n",
      "|         2019-10-31|1787277|\n",
      "|         2019-09-30|1763391|\n",
      "|         2019-08-31|1856202|\n",
      "|         2019-07-31|1839005|\n",
      "|         2019-06-30|1822001|\n",
      "|         2019-05-31|1806604|\n",
      "|         2019-04-30|1788444|\n",
      "|         2019-03-31|1770015|\n",
      "|         2019-02-28|1752593|\n",
      "|         2019-01-31|1738794|\n",
      "|         2018-12-31|1726017|\n",
      "|         2018-11-30|1713615|\n",
      "|         2018-10-31|1699510|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_payoff.groupBy('servicecalendardate').count().sort(desc('servicecalendardate')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|servicecalendardate|  count|\n",
      "+-------------------+-------+\n",
      "|         2020-05-31|1889579|\n",
      "|         2020-04-30|1899099|\n",
      "|         2020-03-31|1867297|\n",
      "|         2020-02-29|1846864|\n",
      "|         2020-01-31|1824706|\n",
      "|         2019-12-31|1802243|\n",
      "|         2019-11-30|1807991|\n",
      "|         2019-10-31|1787277|\n",
      "|         2019-09-30|1763391|\n",
      "|         2019-08-31|1856202|\n",
      "|         2019-07-31|1839005|\n",
      "|         2019-06-30|1822001|\n",
      "|         2019-05-31|1806604|\n",
      "|         2019-04-30|1788444|\n",
      "|         2019-03-31|1770015|\n",
      "|         2019-02-28|1752593|\n",
      "|         2019-01-31|1738794|\n",
      "|         2018-12-31|1726017|\n",
      "|         2018-11-30|1713615|\n",
      "|         2018-10-31|1699510|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_final = build_targets(df_payoff)\n",
    "\n",
    "df_final.groupBy('servicecalendardate').count().sort(desc('servicecalendardate')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_final.write.parquet(actual_path + 'Actual_payoff_20200610.parquet', mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df_final = spark.read.parquet(actual_path + 'Actual_payoff_20200505.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|servicecalendardate|  count|\n",
      "+-------------------+-------+\n",
      "|         2020-04-30|1899099|\n",
      "|         2020-03-31|1867297|\n",
      "|         2020-02-29|1846864|\n",
      "|         2020-01-31|1824706|\n",
      "|         2019-12-31|1802243|\n",
      "|         2019-11-30|1807991|\n",
      "|         2019-10-31|1787277|\n",
      "|         2019-09-30|1763391|\n",
      "|         2019-08-31|1856202|\n",
      "|         2019-07-31|1839005|\n",
      "|         2019-06-30|1822001|\n",
      "|         2019-05-31|1806604|\n",
      "|         2019-04-30|1788444|\n",
      "|         2019-03-31|1770015|\n",
      "|         2019-02-28|1752593|\n",
      "|         2019-01-31|1738794|\n",
      "|         2018-12-31|1726017|\n",
      "|         2018-11-30|1713615|\n",
      "|         2018-10-31|1699510|\n",
      "|         2018-09-30|1683583|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df_final.groupBy('servicecalendardate').count().sort(desc('servicecalendardate')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
